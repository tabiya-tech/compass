import json
from textwrap import dedent
import logging
from typing import Optional


from app.agent.experience import WorkType
from app.agent.llm_caller import LLMCaller
from app.agent.prompt_template.format_prompt import replace_placeholders_with_indent
from app.countries import Country, get_country_glossary
from app.vector_search.esco_entities import SkillEntity
from common_libs.llm.models_utils import LLMConfig, JSON_GENERATION_CONFIG, MODERATE_TEMPERATURE_GENERATION_CONFIG
from common_libs.llm.generative_models import GeminiGenerativeLLM
from evaluation_tests.conversation_libs.evaluators.evaluation_result import EvaluationResult


class ExperienceSummarizerEvalutionOutput(EvaluationResult):
    """
    The output from the ExperienceSummarizerEvaluator.
    Contains the evaluation of the summary and reasoning behind it.
    """
    evaluator_name: str = "Experience Summarizer Evaluator"
    """The name of the evaluator"""

    meets_requirements: bool
    """Whether the summary meets the core task requirements"""

    class Config:
        extra = "forbid"


class ExperienceSummarizerEvaluator:

    def __init__(self, country_of_user: Country):
        self._logger = logging.getLogger(self.__class__.__name__)
        self._llm_caller: LLMCaller[ExperienceSummarizerEvalutionOutput] = LLMCaller[ExperienceSummarizerEvalutionOutput](
            model_response_type=ExperienceSummarizerEvalutionOutput)
        self._llm = GeminiGenerativeLLM(
            system_instructions=ExperienceSummarizerEvaluator.get_system_instructions(country_of_user=country_of_user),
            config=LLMConfig(language_model_name="gemini-2.5-pro",
                             generation_config=MODERATE_TEMPERATURE_GENERATION_CONFIG | JSON_GENERATION_CONFIG)
        )

    @staticmethod
    def get_system_instructions(*, country_of_user: Country) -> str:
        _country_instructions = ""
        if country_of_user is not None and country_of_user != Country.UNSPECIFIED:
            _glossary = get_country_glossary(country_of_user)

            _country_instructions = dedent("""\
            # Country of User
                The country of the user is {country_of_user_name}.
                Use this glossary to understand the terminology and context of the user's experience:
                    {glossary}
            """)
            _country_instructions = replace_placeholders_with_indent(
                _country_instructions,
                country_of_user_name=country_of_user.name,
                glossary=_glossary
            )

        _summarize_system_instructions = dedent("""\
                <System Instructions>
                You are an expert evaluator assessing the quality of a summary generated by a CV summarization model.
        
                # Task
                    Your task is to evaluate whether the summary provided by the model accurately and effectively captures the user’s work/livelihood experience as described in the input. 
                    Assess the summary based on the following criteria:
                        - Relevance: Does the summary focus only on the user’s professional/work experience, avoiding personal details?
                        - Accuracy: Does the summary reflect the key responsibilities and top skills mentioned in the input?
                        - Conciseness: Is the summary well-written and under 100 words?
                        - Clarity & Tone: Is the summary clear, readable, and appropriate for use in a professional CV?
        
                You will use the same input the model received (experience title, company, work type, responsibilities, top skills, and Q&A) along with the output summary provided by the model. 
                {country_instructions}
                # Input Structure
                    The input structure is composed of:
                        'Experience Title': The title of the experience
                        'Company/Receiver of work': The name of the company where the user worked or the receiver of the work in case of volunteering or caregiving.
                        'Work Type': The type of work the user did in this experience.
                        'Responsibilities': The list of responsibilities/activities/skills/behaviours of the user in this experience.
                        'Top skills': The list of top skills the user has used in this experience.
                        'Questions & Answers': A list of questions the user was asked about their experience and the user's answers to those questions.
                        'LLM Output Summary': The text summary produced by the LLM being evaluated.
                        'LLM Output Summary Length': The number of words in the LLM output summary.
        
                # JSON Output instructions
                    You will respond with a JSON object that contains the following fields:
                        - reasoning: A free-text explanation of your judgment on how well the summary aligns with the task instructions, including any strengths or areas for improvement.
                        - score: A numerical rating from 0 to 100 based on the overall quality and alignment of the summary, where:
                                <=50 : Poor
                                 >50 : Fair
                                 >60 : Good
                                 >80 : Very Good
                                 >90 : Excellent
                        - meets_requirements: A boolean field indicating whether the summary meets the core task requirements.
        
                Your response must always be a JSON object with the schema above.
            </System Instructions>
            """)
        return replace_placeholders_with_indent(
            _summarize_system_instructions,
            country_instructions=_country_instructions
        )

    @staticmethod
    def get_prompt(*,
                   experience_title: str,
                   company: Optional[str] = None,
                   work_type: WorkType,
                   responsibilities: list[str],
                   top_skills: Optional[list[SkillEntity]] = None,
                   questions_and_answers: list[tuple[str, str]],
                   llm_summary: str = ""
                   ) -> str:
        _prompt = dedent("""\
            <Input>
                Experience Title: '{experience_title}'
                Company/Receiver of work: '{company}'
                Work Type: '{work_type}'
                Responsibilities: '{responsibilities}'
                Top Skills: {skills}
                Questions & Answers: {questions_and_answers}
                LLM Output Summary: '{llm_summary}'
                LLM Output Summary Length: {llm_summary_length}
            </Input>
            """)

        _skills = "N/A"
        if top_skills is not None and len(top_skills) > 0:
            _skills = [{f'skill title': skill.preferredLabel, f'skill description': skill.description} for skill in top_skills]
            _skills = json.dumps(_skills, indent=4, ensure_ascii=False)

        _questions_and_answers = []
        for question, answer in questions_and_answers:
            _qa_fragment = "Question: '{question}'\nAnswer: '{answer}'\n\n"
            _qa_fragment = replace_placeholders_with_indent(
                _qa_fragment,
                question=question,
                answer=answer
            )
            _questions_and_answers.append(_qa_fragment)

        _questions_and_answers = '\n'.join(_questions_and_answers)

        return replace_placeholders_with_indent(
            _prompt,
            experience_title=experience_title,
            company=company if company else "N/A",
            work_type=WorkType.work_type_short(work_type) if work_type else "N/A",
            responsibilities=', '.join(responsibilities) if responsibilities else "N/A",
            skills=_skills,
            questions_and_answers=_questions_and_answers,
            llm_summary=llm_summary,
            llm_summary_length=str(len(llm_summary.split()))  # Count words for length
        )

    async def evaluate(
            self, *,
            experience_title: str,
            company: Optional[str] = None,
            work_type: WorkType,
            responsibilities: list[str],
            top_skills: Optional[list[SkillEntity]] = None,
            questions_and_answers: list[tuple[str, str]],
            llm_summary: str = ""
    ) -> ExperienceSummarizerEvalutionOutput:
        prompt = ExperienceSummarizerEvaluator.get_prompt(
            experience_title=experience_title,
            company=company,
            work_type=work_type,
            responsibilities=responsibilities,
            top_skills=top_skills,
            questions_and_answers=questions_and_answers,
            llm_summary=llm_summary,
        )

        llm_response, llm_stats = await self._llm_caller.call_llm(
            llm=self._llm,
            llm_input=prompt,
            logger=self._logger
        )
        # Add the experience title to the evaluator name for clarity in the output
        llm_response.evaluator_name += f" [{experience_title}]"
        return llm_response
