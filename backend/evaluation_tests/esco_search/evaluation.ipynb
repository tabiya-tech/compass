{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n",
    "\n",
    "The purpose of this notebook is to evaluate the vector search component of our RAG model. In order to do so, we consider two pre-processed datasets respectively for job and skills data. The first one contains 552 job titles, descriptions and synthetic queries that a Brujula user might submit to the platform, as well as a ground truth ESCO code for occupation and its related essential skills, contained as a list of Tabiya UUIDs for skills. Further information can be found in the [Hahu test Huggingface repository](https://huggingface.co/datasets/tabiya/hahu_test). The second dataset contains 1054 sentences with multiple skills (2013 in total) which are extracted from job description requirements. In this case, we also generated a synthetic query for the test input to match closer our use case. Further information can be found in the [Skill test set Huggingface repository](https://huggingface.co/datasets/tabiya/esco_skills_test). \n",
    "\n",
    "In order to set an evaluation target, let us consider our downstream application. In fact, the ESCO nodes found by our vector search will be used to build a prompt so that the Large Language Model can ask users to validate a number of skills and occupations found through vector search. Because of this two step process, we don't want to focus so much on the precision (that is, reducing the amount of false positives), but rather on the recall (that is, retrieving the largest amount of correct nodes). We also envision a reranking component that will be capable of highlighting the most important skills for a measure that could be defined externally (based, for instance, on the intrinsic value of various skills) or internally (if the user has mentioned related nodes multiple times).\n",
    "\n",
    "\n",
    "On this premise, we focus our evaluation method on the recall@k, which is a metric that measures how many correct nodes are found within the top k classes. The precision@k, which tells us how many of the k retrieved nodes are correct, will be a secondary metric to consider to check how many false positives we retrieve, penalizing the use of larger sample sizes. The two metrics are summarized by the F-score@k. Each of these metrics could be further refined by considering the score to be inversely proportional to the rank in which the correct node is found. However, since our first iteration is not concerned with the rank, we will use a 0-1 score.\n",
    "\n",
    "In the case of occupations, since our test set includes only one correct node per sentences, the recall@k reduces to understanding whether the correct node is within the first k elements. However, multiple skill nodes can correspond to a single sentence, so that the recall at k really captures what percentage of the correct skills on average is retrieved within the first k elements, as we take one sentence per data-point.\n",
    "\n",
    "We start by defining our metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "def precision_at_k(prediction: List[List[str]], true: List[List[str]], k: Optional[int] = None):\n",
    "    \"\"\"Calculates the average precision at k considering\n",
    "    for each prediction the number of correct retrieved nodes\n",
    "    divided by the number of total retrieved nodes.\n",
    "\n",
    "    Args:\n",
    "        prediction (List[List[str]]): list of \n",
    "            predicted lists, each with the corresponding\n",
    "            nodes.\n",
    "        true (List[List[str]]): list of the multiple true nodes \n",
    "            for each sample in the dataset.\n",
    "        k (Optional[int]): number of samples of the prediction to consider.\n",
    "            When None considers all the elements of the list.\n",
    "\n",
    "    Returns:\n",
    "        float: average precision at k over all the test set.\n",
    "    \"\"\"\n",
    "    assert len(prediction) == len(true)\n",
    "    total_precision = 0\n",
    "    for pred_list, true_val in zip(prediction, true):\n",
    "        if k:\n",
    "            pred_list = pred_list[:k]\n",
    "            tot_samples = k\n",
    "        else:\n",
    "            tot_samples = len(pred_list)\n",
    "        total_precision+=len(set(pred_list).intersection(set(true_val)))/tot_samples\n",
    "    return total_precision/len(true)\n",
    "\n",
    "def recall_at_k(prediction: List[List[str]], true: List[List[str]], k: Optional[int] = None):\n",
    "    \"\"\"Calculates the average recall at k considering\n",
    "    for each prediction the number of correct retrieved nodes\n",
    "    divided by the number of total correct nodes.\n",
    "\n",
    "    Args:\n",
    "        prediction (List[List[str]]): list of \n",
    "            predicted lists, each with the corresponding\n",
    "            nodes.\n",
    "        true (List[List[str]]): list of the multiple true nodes \n",
    "            for each sample in the dataset.\n",
    "        k (Optional[int]): number of predicted samples to consider.\n",
    "            When None considers all of them. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: average recall at k over all the test set.\n",
    "    \"\"\"\n",
    "    assert len(prediction) == len(true)\n",
    "    total_recall = 0\n",
    "    for pred_list, true_val in zip(prediction, true):\n",
    "        if k:\n",
    "            pred_list = pred_list[:k]\n",
    "        total_recall+=len(set(pred_list).intersection(set(true_val)))/len(set(true_val))\n",
    "    return total_recall/len(true)\n",
    "\n",
    "def f_score(prec: float, rec: float) -> float:\n",
    "    \"\"\"Returns the f-score corresponding to\n",
    "    a given precision and recall.\n",
    "\n",
    "    Args:\n",
    "        prec (float): provided precision\n",
    "        rec (float): provided recall\n",
    "\n",
    "    Returns:\n",
    "        float: resulting f-score\n",
    "    \"\"\"\n",
    "    if prec+rec!=0:\n",
    "        return 2*prec*rec/(prec+rec)\n",
    "    return 0\n",
    "\n",
    "def get_all_metrics(predictions: List[List[str]], true_values: List[List[str]], k: Optional[int]=None) -> Tuple[float,float,float]:\n",
    "    \"\"\"Get recall, precision and F-score for given results and\n",
    "    true values.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[List[str]]): list of predictions.\n",
    "        true_values (List[List[str]]): list of true values.\n",
    "        k (Optional[int]): number of predicted samples to consider.\n",
    "            When None considers all of them. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float,float,float]: recall, precision and F-score.\n",
    "    \"\"\"\n",
    "    rec_at_k = recall_at_k(predictions, true_values, k)\n",
    "    prec_at_k = precision_at_k(predictions, true_values, k)\n",
    "    f_score_at_k = f_score(prec_at_k, rec_at_k)\n",
    "    return rec_at_k, prec_at_k, f_score_at_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation goals\n",
    "\n",
    "We aim to run multiple evaluations on different goals, fixing the embedding model we're using as the Vertex AI Gecko003 model. We consider other variables in our experiment setting them as hyperparameters. In particular we would like to know:\n",
    "\n",
    "1. Which hyperparameter we should choose to properly retrieve a node.\n",
    "2. How we can retrieve skills related to a query concerning a job.\n",
    "3. Whether job titles are better indicators than job descriptions when retrieving the correct information.\n",
    "4. Whether using multiple embeddings can guarantee a higher performance.\n",
    "5. How many nodes we need to retrieve to guarantee a recall of 1 on the test set.\n",
    "6. Which method can apply to localised ESCO data (for which we will use an appropriate test set).\n",
    "\n",
    "The corresponding test datasets will be loaded from the aforementioned Huggingface respositories, while the skill, occupation and occupation-skill relational databases will be loaded from the public Tabiya Github repository [tabiya-open-dataset](https://github.com/tabiya-tech/tabiya-open-dataset/tree/main/tabiya-esco-v1.1.1). \n",
    "\n",
    "The following code loads the datasets, defines a series of functions useful to all three evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading the test dataset for occupations using the Huggingface library\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the environment variables to access Huggingface and Google Vertex API\n",
    "HF_TOKEN = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "GOOGLE_PROJECT_ID = os.environ[\"GOOGLE_PROJECT_ID\"]\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "\n",
    "OCCUPATION_REPO_ID = \"tabiya/hahu_test\"\n",
    "OCCUPATION_FILENAME = \"redacted_hahu_test_with_id.csv\"\n",
    "SKILL_REPO_ID = \"tabiya/esco_skills_test\"\n",
    "SKILL_FILENAME = \"data/processed_skill_test_set_with_id.parquet\"\n",
    "LOCALISED_SA_REPO_ID = \"tabiya/localised_esco_dataset_sa\"\n",
    "LOCALISED_ESCO_FILENAME = \"esco_occupations_fc_220424.csv\"\n",
    "LOCALISED_TEST_FILENAME = \"sa_test_set.csv\"\n",
    "OCCUPATION_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/refs/heads/main/data-sets/csv/esco-1.1.1%20v1.0.0/occupations.csv\"\n",
    "SKILL_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/refs/heads/main/data-sets/csv/esco-1.1.1%20v1.0.0/skills.csv\"\n",
    "OCCUPATION_TO_SKILL_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/refs/heads/main/data-sets/csv/esco-1.1.1%20v1.0.0/occupation_to_skill_relations.csv\"\n",
    "FR_OCCUPATION_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/refs/heads/main/data-sets/csv/esco-v1.1.1(fr)/occupations.csv\"\n",
    "FR_OCCUPATION_FILENAME = \"synthetic_queries_translated.csv\"\n",
    "\n",
    "\n",
    "\n",
    "df_occupation_to_skills = pd.read_csv(OCCUPATION_TO_SKILL_DATA_PATH)\n",
    "\n",
    "df_occupation_test = pd.read_csv(\n",
    "    hf_hub_download(repo_id=OCCUPATION_REPO_ID, filename=OCCUPATION_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "df_skill_test = pd.read_parquet(\n",
    "    hf_hub_download(repo_id=SKILL_REPO_ID, filename=SKILL_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "df_occupation_database = pd.read_csv(OCCUPATION_DATA_PATH)\n",
    "df_skill_database = pd.read_csv(SKILL_DATA_PATH)\n",
    "sa_test_df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=LOCALISED_SA_REPO_ID, filename=LOCALISED_TEST_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "sa_occupation_database_df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=LOCALISED_SA_REPO_ID, filename=LOCALISED_ESCO_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "fr_test_df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=OCCUPATION_REPO_ID, filename=FR_OCCUPATION_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "df_occupation_database_fr = pd.read_csv(FR_OCCUPATION_DATA_PATH)\n",
    "\n",
    "# Before calling the model, make sure that the GOOGLE_PROJECT_ID\n",
    "# and GOOGLE_APPLICATION_CREDENTIALS environment variables are set\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize skills and occupation test target fields with the database targets\n",
    "df_skill_database = df_skill_database.rename(columns={\"UUIDHISTORY\":\"CODE\"})\n",
    "\n",
    "# We transform the skill dataset to contain only unique sentences and corresponding lists of skill UUIDs\n",
    "uuid_grouped = df_skill_test.groupby('synthetic_query')['UUID'].agg(list).reset_index()\n",
    "df_skill_test = df_skill_test.merge(uuid_grouped, on='synthetic_query', suffixes=('', '_CODE'))\n",
    "df_skill_test.rename(columns={'UUID_CODE': 'CODE'}, inplace=True)\n",
    "df_skill_test = df_skill_test.drop_duplicates(subset=['synthetic_query'])\n",
    "df_skill_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_occupation_test[\"CODE\"] = df_occupation_test[\"esco_code\"].apply(lambda x: [x])\n",
    "df_occupation_test[\"skills_essential\"] = df_occupation_test[\"skills_essential\"].apply(eval)\n",
    "df_occupation_test[\"skills_optional\"] = df_occupation_test[\"skills_optional\"].apply(eval)\n",
    "sa_test_df[\"CODE\"] = sa_test_df[\"Esco Code\"].apply(lambda x: [str(x)])\n",
    "fr_test_df[\"fr_to_en_synthetic_query\"] = fr_test_df[\"fr_to_en_synthetic_query\"].apply(str)\n",
    "fr_test_df[\"CODE\"] = fr_test_df[\"esco_code\"].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \"maximal_marginal_relevance\" and \"cosine_similarity\"\n",
    "# are duplicated respectively from modules:\n",
    "#    - \"libs/community/langchain_community/vectorstores/utils.py\"\n",
    "#    - \"libs/community/langchain_community/utils/math.py\"\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Matrix = Union[List[List[float]], List[np.ndarray], np.ndarray]\n",
    "\n",
    "\n",
    "def cosine_similarity(X: Matrix, Y: Matrix) -> np.ndarray:\n",
    "    \"\"\"Row-wise cosine similarity between two equal-width matrices.\"\"\"\n",
    "    if len(X) == 0 or len(Y) == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Number of columns in X and Y must be the same. X has shape {X.shape} \"\n",
    "            f\"and Y has shape {Y.shape}.\"\n",
    "        )\n",
    "    X_norm = np.linalg.norm(X, axis=1)\n",
    "    Y_norm = np.linalg.norm(Y, axis=1)\n",
    "    # Ignore divide by zero errors run time warnings as those are handled below.\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n",
    "    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "def maximal_marginal_relevance(\n",
    "    query_embedding: np.ndarray,\n",
    "    embedding_list: list,\n",
    "    lambda_mult: float = 0.5,\n",
    "    k: int = 10,\n",
    ") -> List[int]:\n",
    "    \"\"\"Calculate maximal marginal relevance.\"\"\"\n",
    "    if min(k, len(embedding_list)) <= 0:\n",
    "        return []\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    similarity_to_query = cosine_similarity(query_embedding, embedding_list)[0]\n",
    "    most_similar = int(np.argmax(similarity_to_query))\n",
    "    idxs = [most_similar]\n",
    "    selected = np.array([embedding_list[most_similar]])\n",
    "    while len(idxs) < min(k, len(embedding_list)):\n",
    "        best_score = -np.inf\n",
    "        idx_to_add = -1\n",
    "        similarity_to_selected = cosine_similarity(embedding_list, selected)\n",
    "        for i, query_score in enumerate(similarity_to_query):\n",
    "            if i in idxs:\n",
    "                continue\n",
    "            redundant_score = max(similarity_to_selected[i])\n",
    "            equation_score = (\n",
    "                lambda_mult * query_score - (1 - lambda_mult) * redundant_score\n",
    "            )\n",
    "            if equation_score > best_score:\n",
    "                best_score = equation_score\n",
    "                idx_to_add = i\n",
    "        idxs.append(idx_to_add)\n",
    "        selected = np.append(selected, [embedding_list[idx_to_add]], axis=0)\n",
    "    return idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we will pre-compute the strings and the corresponding embeddings using the Gecko model. We will use manual batching to speed up the process, as the vertex API doesn't support batching and fails if the list length is larger than 250 elements or the sum of tokens is higher than 20.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_strings_in_batch(list_of_queries: List[str], model: TextEmbeddingModel, batch_size: int = 100) -> List[List[float]]:\n",
    "    \"\"\"Uses a TextEmbeddingModel to embed a list of queries in batches.\n",
    "\n",
    "    Args:\n",
    "        list_of_queries (List[str]): list of queries to be embedded in batches.\n",
    "        model (TextEmbeddingModel): embedding model.\n",
    "        batch_size (int, optional): size of each batch which should be less than or equal to 250.\n",
    "            Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: List of embeddings corresponding to the queries.\n",
    "    \"\"\"\n",
    "    assert batch_size<=250\n",
    "    embedding_results = []\n",
    "    num_samples = len(list_of_queries)\n",
    "    for i in range(int(num_samples/batch_size)+1):\n",
    "        batch = list_of_queries[i*batch_size:(i+1)*batch_size]\n",
    "        embedding_results += model.get_embeddings(batch)\n",
    "    assert len(embedding_results) == len(list_of_queries)\n",
    "    return [embedding_result.values for embedding_result in embedding_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter selection\n",
    "\n",
    "The objective of this first evaluation is to choose the hyperparameters which guarantee the highest recall at each k. We select a combination of the following hyperparameters for k equal to 1, 3, 5 and 10:\n",
    "\n",
    "1. **How to embed a node of the graph**: which combination of the fields guarantees the best performance when embedded. We consider embedding only the **preferred label**, only the **description**, the **label and description** or the **label, description and secondary labels**.\n",
    "2. **Score function**: which function should be used to retrieve the most similar nodes (*cosine*, *l2 distance* or *scalar product*).\n",
    "3. **Using Maximal Marginal Relevance**: whether we should use **MMR** to get more diverse results. Maximal marginal relevance is an optimization algorithm that for a given subset of retrieved nodes chooses the best and most diverse nodes in terms of similarity with each other.\n",
    "\n",
    "We will use ChromaDB as a local vector store and get the ESCO data from a local csv file. We will restrict our evaluation to the gecko003 model, but this can be repeated with any other model.\n",
    "\n",
    "The first evaluation will be conducted as follows:\n",
    "\n",
    "- Evaluation of the occupation nodes from the occupation dataset.\n",
    "- Evaluation of the skill nodes from the skill dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions defining strings to embed\n",
    "def description(df):\n",
    "    return df[\"DESCRIPTION\"]\n",
    "\n",
    "def preferred_label(df):\n",
    "    return df[\"PREFERREDLABEL\"]\n",
    "\n",
    "def all_occupations(df):\n",
    "    return f\"\"\"Occupation Names: {df['PREFERREDLABEL']}\n",
    "{df['ALTLABELS']}\n",
    "\n",
    "Occupation Description: {df['DESCRIPTION']}\"\"\"\n",
    "\n",
    "def all_skills(df):\n",
    "    return f\"\"\"Skill Names: {df['PREFERREDLABEL']}\n",
    "{df['ALTLABELS']}\n",
    "\n",
    "Skill Description: {df['DESCRIPTION']}\"\"\"\n",
    "\n",
    "def label_and_description(df):\n",
    "    return f\"{df['PREFERREDLABEL']}\\n{df['DESCRIPTION']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding precomputation\n",
    "\n",
    "def precompute_embeddings(df_database: pd.DataFrame, function_to_method: Dict[str,Any]) -> pd.DataFrame:\n",
    "    \"\"\"For a given database and map that sends each function name to\n",
    "    its respective function for selecting a substring from the node,\n",
    "    returns an updated dataframe with the corresponding methods as\n",
    "    well as embeddings for all the methods\n",
    "\n",
    "    Args:\n",
    "        df_database (pd.DataFrame): database of interest\n",
    "        function_to_method (Dict[str,Any]): map from function\n",
    "            name to function selecting string for any given node.\n",
    "\n",
    "    Returns:\n",
    "        The updated dataframe with the method strings and the corresponding\n",
    "        embeddings.\n",
    "    \"\"\"\n",
    "    for method in function_to_method:\n",
    "        df_database[method] = df_database.progress_apply(function_to_method[method], axis=1)\n",
    "        embeddings = embed_strings_in_batch(list(df_database[method]), model)\n",
    "        df_database[f'embeddings_{method}'] = embeddings\n",
    "    return df_database\n",
    "\n",
    "function_to_occupation_method = {\"DESCRIPTION\": description, \"PREFERREDLABEL\":preferred_label, \"ALL_OCCUPATIONS\":all_occupations, \"LABEL_AND_DESCRIPTION\": label_and_description}\n",
    "function_to_skill_method = {\"DESCRIPTION\": description, \"PREFERREDLABEL\":preferred_label, \"ALL_SKILLS\":all_skills, \"LABEL_AND_DESCRIPTION\": label_and_description}\n",
    "\n",
    "# Compute database embeddings\n",
    "df_occupation_database = precompute_embeddings(df_occupation_database, function_to_occupation_method)\n",
    "df_skill_database = precompute_embeddings(df_skill_database, function_to_skill_method)\n",
    "\n",
    "# Compute test set embeddings\n",
    "test_occupation_embeddings = embed_strings_in_batch(list(df_occupation_test[\"synthetic_query\"]), model)\n",
    "df_occupation_test[\"embeddings\"] = test_occupation_embeddings\n",
    "\n",
    "test_skill_embeddings = embed_strings_in_batch(list(df_skill_test[\"synthetic_query\"]), model)\n",
    "df_skill_test[\"embeddings\"] = test_skill_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create multiple chromadb collections to store our data in memory with different embeddings depending on the method used and on the function used for querying. On these, we save the Occupation ESCO database with all their metadatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "SCORE_FUNCTIONS = [\"cosine\", \"l2\", \"ip\"]\n",
    "client = chromadb.Client()\n",
    "\n",
    "def create_collection_in_batch(\n",
    "        db_name: str,\n",
    "        df_database: pd.DataFrame,\n",
    "        batch_size: int = 41655,\n",
    "        collection_metadata: Optional[Dict[str,Any]] = None,\n",
    "        text_column: str =\"text\",\n",
    "        embedding_column: str = \"embeddings\"\n",
    "        ):\n",
    "    \"\"\"Creates a collection for a db_name\n",
    "    and corresponding documents and embeddings. Can be used\n",
    "    for large databases so that the collection is created in batch.\n",
    "\n",
    "    Args:\n",
    "        db_name (str): name of the database. \n",
    "            Either 'skills' or 'occupations'.\n",
    "        df_database (pd.DataFrame): database containing the metadata.\n",
    "        batch_size (int): size of the batch to create the collection.\n",
    "            Defaults to 41655.\n",
    "        collection_metadata (Optional[Dict[str,Any]]): metadata to be saved in the collection.\n",
    "            Defaults to None.\n",
    "        text_column (str): column of the dataframe containing the text of interest.\n",
    "            Defaults to 'text'.\n",
    "        embedding_column (str): column of the dataframe containing the embeddings.\n",
    "            Defaults to 'embeddings'.\n",
    "\n",
    "    \"\"\"\n",
    "    if collection_metadata is not None:\n",
    "        collection = client.create_collection(name=db_name, metadata=collection_metadata)\n",
    "    else:\n",
    "        collection = client.create_collection(name=db_name)\n",
    "    batch_number = int(len(df_database)/batch_size)+1\n",
    "    for i in range(batch_number):\n",
    "        temp_database = df_database.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        collection.add(\n",
    "            documents = list(temp_database[text_column]),\n",
    "            metadatas = [{\"CODE\": row[\"CODE\"]} for _, row in temp_database.iterrows()],\n",
    "            embeddings = list(temp_database[embedding_column]),\n",
    "            ids = [f\"id_{i*41655+j}\" for j in range(len(temp_database))]\n",
    "        )\n",
    "\n",
    "def create_collections(db_name: str, methods: List[str], df_database: pd.DataFrame):\n",
    "    \"\"\"Creates multiple collections for each choice of db_name\n",
    "    and corresponding documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        db_name (str): name of the database. \n",
    "            Either 'skills' or 'occupations'.\n",
    "        methods (List[str]): list of methods for the embeddings.\n",
    "        df_database (pd.DataFrame): database containing the metadata.\n",
    "    \"\"\"\n",
    "    for method in methods:\n",
    "        for score in SCORE_FUNCTIONS:\n",
    "            collection_name = f'{db_name}_{method}_{score}'\n",
    "            collection_metadata = {\"hnsw:space\":score}\n",
    "            create_collection_in_batch(\n",
    "                collection_name,\n",
    "                df_database,\n",
    "                collection_metadata=collection_metadata,\n",
    "                text_column=method,\n",
    "                embedding_column=f\"embeddings_{method}\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_collections(\"occupations\", list(function_to_occupation_method.keys()), df_occupation_database)\n",
    "create_collections(\"skills\", list(function_to_skill_method.keys()), df_skill_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write a function to run the evaluation. The one linking skills to skills and occupations to occupations works as follows:\n",
    "\n",
    "1. We choose a score function and a method and load the corresponding collection.\n",
    "2. For each element in the test set, we find the top 100 documents in the collection ordered by scoring rank.\n",
    "3. We filter those documents by maximal marginal relevance to find the top 10 documents with this function.\n",
    "4. We evaluate the precision, recall and F-score on the top k for k=1,3,5,10 for the original retrieved documents.\n",
    "5. We evaluate the precision, recall and F-score on the top k for k=1,3,5,10 for the documents filtered by maximal marginal relevance.\n",
    "6. We save the results in a dataframe to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_results_from_embeddings(embeddings: List[List[float]], collection: chromadb.Collection, n_results:int=100, mmr: bool = False) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"Utility function to return results of embedding queries\n",
    "    to a given collection.\n",
    "\n",
    "    Args:\n",
    "        embeddings (List[List[float]]): List of embeddings for queries.\n",
    "        collection (chromadb.Collection): ChromaDB collection to query.\n",
    "        n_results (int): number of results to retrieve from the collection.\n",
    "            Defaults to 100.\n",
    "        mmr (bool): whether we want the result to be filtered by Maximal Marginal Relevance.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of results, either for regular vector search, \n",
    "            or for maximal marginal relevance search. Each element is a list of \n",
    "            string corresponding to the search result for the embedding in the \n",
    "            same position in the input list. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for embedding in embeddings:\n",
    "        # Find the top 100 documents and save them in vector_search_results\n",
    "        documents_from_search = collection.query(query_embeddings=embedding, n_results=n_results, include=[\"metadatas\", \"embeddings\"])\n",
    "        if mmr:\n",
    "            result_embeddings = [elem for elem in documents_from_search[\"embeddings\"][0]]\n",
    "            mmr_ids = maximal_marginal_relevance(np.array(embedding), result_embeddings)\n",
    "            results.append([elem[\"CODE\"] for index, elem in enumerate(documents_from_search[\"metadatas\"][0]) if index in mmr_ids])\n",
    "        else:\n",
    "            results.append([elem[\"CODE\"] for elem in documents_from_search[\"metadatas\"][0]])\n",
    "    return results\n",
    "\n",
    "def get_results_from_embeddings(embeddings: List[List[float]], collection: chromadb.Collection) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"Utility function to return results of embedding queries\n",
    "    to a given collection.\n",
    "\n",
    "    Args:\n",
    "        embeddings (List[List[float]]): List of embeddings for queries.\n",
    "        collection (chromadb.Collection): ChromaDB collection to query.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[str]], List[List[str]]]: List of results, one for\n",
    "            regular vector search, the other one for maximal marginal relevance\n",
    "            search. Each element is a list of string corresponding to the\n",
    "            search result for the embedding in the same position in the input list. \n",
    "    \"\"\"\n",
    "    vector_search_results = get_top_n_results_from_embeddings(embeddings, collection)\n",
    "    mmr_vector_search_results = get_top_n_results_from_embeddings(embeddings, collection, mmr=True)\n",
    "    return vector_search_results, mmr_vector_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_for_multiple_collections(db_type: str, method_list: List[str], score_function_list: List[str], df_test: pd.DataFrame, target_column: str = \"CODE\", embedding_column = \"embeddings\") -> pd.DataFrame:\n",
    "    \"\"\"Returns the results of an evaluation on a list of collections\n",
    "\n",
    "    Args:\n",
    "        db_type (str): name of the database (occupation or skill).\n",
    "        method_list (List[str]): list of methods to be tested.\n",
    "        score_function_list (List[str]): list of score functions to be tested.\n",
    "        df_test (pd.DataFrame): test dataframe, containing an embedding column\n",
    "            and a test_target column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the result of the evaluation depending on the\n",
    "            different hyperparameters.\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    for method in method_list:\n",
    "        for score in score_function_list:\n",
    "            collection_name = f\"{db_type}_{method}_{score}\"\n",
    "            # Fetch collection\n",
    "            collection = client.get_collection(name=collection_name)\n",
    "            # Initialize lists to save results\n",
    "            vector_search_results, mmr_vector_search_results = get_results_from_embeddings(list(df_test[embedding_column]), collection)\n",
    "            # Evaluate accuracy at k for k=1, 3, 5, 10\n",
    "            for k in [1, 3, 5, 10]:\n",
    "                rec_at_k, prec_at_k, f_score_at_k = get_all_metrics(vector_search_results, list(df_test[target_column]), k)\n",
    "                eval_data.append({\"method\":method, \"score function\":score, \"MMR\": False, \"k\":k, \"recall\": round(rec_at_k, 4), \"precision\": round(prec_at_k,4), \"f-score\": round(f_score_at_k,4)})\n",
    "                rec_at_k, prec_at_k, f_score_at_k = get_all_metrics(mmr_vector_search_results, list(df_test[target_column]), k)\n",
    "                eval_data.append({\"method\":method, \"score function\":score, \"MMR\": True, \"k\":k, \"recall\": round(rec_at_k, 4), \"precision\": round(prec_at_k,4), \"f-score\": round(f_score_at_k,4)})\n",
    "    # Save the results in a dataframe\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    return eval_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the evaluations for occupation and skill vector search. The script can be modified to save the results locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of occupation and skills. Modify the notebook to save the results locally.\n",
    "\n",
    "df_occupation_eval = run_eval_for_multiple_collections(\"occupations\", list(function_to_occupation_method.keys()), SCORE_FUNCTIONS, df_occupation_test)\n",
    "df_skill_eval = run_eval_for_multiple_collections(\"skills\", list(function_to_skill_method.keys()), SCORE_FUNCTIONS, df_skill_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss the results of our experiments\n",
    "\n",
    "### Occupations\n",
    "\n",
    "The following table illustrates the result of our experiments:\n",
    "\n",
    "| Method              | Score Function | MMR   | k  | Recall | Precision | F-Score |\n",
    "|---------------------|----------------|-------|----|--------|-----------|---------|\n",
    "| PREFERREDLABEL      | cosine         | False | 10 | 0.7454 | 0.0745    | 0.1355  |\n",
    "| PREFERREDLABEL      | l2             | False | 10 | 0.7454 | 0.0745    | 0.1355  |\n",
    "| PREFERREDLABEL      | ip             | False | 10 | 0.7454 | 0.0745    | 0.1355  |\n",
    "| ALL_OCCUPATIONS     | cosine         | False | 10 | 0.738  | 0.0738    | 0.1342  |\n",
    "| ALL_OCCUPATIONS     | l2             | False | 10 | 0.738  | 0.0738    | 0.1342  |\n",
    "| ALL_OCCUPATIONS     | ip             | False | 10 | 0.738  | 0.0738    | 0.1342  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | False | 10 | 0.7196 | 0.072     | 0.1308  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | False | 10 | 0.7196 | 0.072     | 0.1308  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | False | 10 | 0.7196 | 0.072     | 0.1308  |\n",
    "| DESCRIPTION         | cosine         | False | 10 | 0.7122 | 0.0712    | 0.1295  |\n",
    "| DESCRIPTION         | l2             | False | 10 | 0.7122 | 0.0712    | 0.1295  |\n",
    "| DESCRIPTION         | ip             | False | 10 | 0.7122 | 0.0712    | 0.1295  |\n",
    "| PREFERREDLABEL      | cosine         | True  | 10 | 0.452  | 0.0452    | 0.0822  |\n",
    "| PREFERREDLABEL      | l2             | True  | 10 | 0.452  | 0.0452    | 0.0822  |\n",
    "| PREFERREDLABEL      | ip             | True  | 10 | 0.452  | 0.0452    | 0.0822  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | True  | 10 | 0.4317 | 0.0432    | 0.0785  |\n",
    "| ALL_OCCUPATIONS     | cosine         | True  | 10 | 0.4299 | 0.043     | 0.0782  |\n",
    "| ALL_OCCUPATIONS     | l2             | True  | 10 | 0.4299 | 0.043     | 0.0782  |\n",
    "| ALL_OCCUPATIONS     | ip             | True  | 10 | 0.4299 | 0.043     | 0.0782  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | True  | 10 | 0.4299 | 0.043     | 0.0782  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | True  | 10 | 0.4299 | 0.043     | 0.0782  |\n",
    "| DESCRIPTION         | cosine         | True  | 10 | 0.3985 | 0.0399    | 0.0725  |\n",
    "| DESCRIPTION         | l2             | True  | 10 | 0.3985 | 0.0399    | 0.0725  |\n",
    "| DESCRIPTION         | ip             | True  | 10 | 0.3985 | 0.0399    | 0.0725  |\n",
    "| PREFERREDLABEL      | cosine         | False | 5  | 0.6624 | 0.1325    | 0.2208  |\n",
    "| PREFERREDLABEL      | l2             | False | 5  | 0.6624 | 0.1325    | 0.2208  |\n",
    "| PREFERREDLABEL      | ip             | False | 5  | 0.6624 | 0.1325    | 0.2208  |\n",
    "| ALL_OCCUPATIONS     | cosine         | False | 5  | 0.6605 | 0.1321    | 0.2202  |\n",
    "| ALL_OCCUPATIONS     | l2             | False | 5  | 0.6605 | 0.1321    | 0.2202  |\n",
    "| ALL_OCCUPATIONS     | ip             | False | 5  | 0.6605 | 0.1321    | 0.2202  |\n",
    "| DESCRIPTION         | cosine         | False | 5  | 0.6181 | 0.1236    | 0.206   |\n",
    "| DESCRIPTION         | l2             | False | 5  | 0.6181 | 0.1236    | 0.206   |\n",
    "| DESCRIPTION         | ip             | False | 5  | 0.6181 | 0.1236    | 0.206   |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | False | 5  | 0.6144 | 0.1229    | 0.2048  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | False | 5  | 0.6144 | 0.1229    | 0.2048  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | False | 5  | 0.6144 | 0.1229    | 0.2048  |\n",
    "| PREFERREDLABEL      | cosine         | True  | 5  | 0.4483 | 0.0897    | 0.1494  |\n",
    "| PREFERREDLABEL      | l2             | True  | 5  | 0.4483 | 0.0897    | 0.1494  |\n",
    "| PREFERREDLABEL      | ip             | True  | 5  | 0.4483 | 0.0897    | 0.1494  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | True  | 5  | 0.4317 | 0.0863    | 0.1439  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | True  | 5  | 0.4299 | 0.086     | 0.1433  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | True  | 5  | 0.4299 | 0.086     | 0.1433  |\n",
    "| ALL_OCCUPATIONS     | cosine         | True  | 5  | 0.4244 | 0.0849    | 0.1415  |\n",
    "| ALL_OCCUPATIONS     | l2             | True  | 5  | 0.4244 | 0.0849    | 0.1415  |\n",
    "| ALL_OCCUPATIONS     | ip             | True  | 5  | 0.4244 | 0.0849    | 0.1415  |\n",
    "| DESCRIPTION         | cosine         | True  | 5  | 0.3967 | 0.0793    | 0.1322  |\n",
    "| DESCRIPTION         | l2             | True  | 5  | 0.3967 | 0.0793    | 0.1322  |\n",
    "| DESCRIPTION         | ip             | True  | 5  | 0.3967 | 0.0793    | 0.1322  |\n",
    "| PREFERREDLABEL      | cosine         | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| PREFERREDLABEL      | l2             | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| PREFERREDLABEL      | ip             | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| ALL_OCCUPATIONS     | cosine         | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| ALL_OCCUPATIONS     | l2             | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| ALL_OCCUPATIONS     | ip             | False | 3  | 0.5812 | 0.1937    | 0.2906  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | False | 3  | 0.548  | 0.1827    | 0.274   |\n",
    "| LABEL_AND_DESCRIPTION | l2           | False | 3  | 0.548  | 0.1827    | 0.274   |\n",
    "| LABEL_AND_DESCRIPTION | ip           | False | 3  | 0.548  | 0.1827    | 0.274   |\n",
    "| DESCRIPTION         | cosine         | False | 3  | 0.5424 | 0.1808    | 0.2712  |\n",
    "| DESCRIPTION         | l2             | False | 3  | 0.5424 | 0.1808    | 0.2712  |\n",
    "| DESCRIPTION         | ip             | False | 3  | 0.5424 | 0.1808    | 0.2712  |\n",
    "| PREFERREDLABEL      | cosine         | True  | 3  | 0.4317 | 0.1439    | 0.2159  |\n",
    "| PREFERREDLABEL      | l2             | True  | 3  | 0.4317 | 0.1439    | 0.2159  |\n",
    "| PREFERREDLABEL      | ip             | True  | 3  | 0.4317 | 0.1439    | 0.2159  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | True  | 3  | 0.4188 | 0.1396    | 0.2094  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | True  | 3  | 0.4188 | 0.1396    | 0.2094  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | True  | 3  | 0.4188 | 0.1396    | 0.2094  |\n",
    "| ALL_OCCUPATIONS     | cosine         | True  | 3  | 0.417  | 0.139     | 0.2085  |\n",
    "| ALL_OCCUPATIONS     | l2             | True  | 3  | 0.417  | 0.139     | 0.2085  |\n",
    "| ALL_OCCUPATIONS     | ip             | True  | 3  | 0.417  | 0.139     | 0.2085  |\n",
    "| DESCRIPTION         | cosine         | True  | 3  | 0.3838 | 0.1279    | 0.1919  |\n",
    "| DESCRIPTION         | l2             | True  | 3  | 0.3838 | 0.1279    | 0.1919  |\n",
    "| DESCRIPTION         | ip             | True  | 3  | 0.3838 | 0.1279    | 0.1919  |\n",
    "| PREFERREDLABEL      | cosine         | False | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| PREFERREDLABEL      | cosine         | True  | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| PREFERREDLABEL      | l2             | False | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| PREFERREDLABEL      | l2             | True  | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| PREFERREDLABEL      | ip             | False | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| PREFERREDLABEL      | ip             | True  | 1  | 0.3801 | 0.3801    | 0.3801  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | False | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| LABEL_AND_DESCRIPTION | cosine       | True  | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | False | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| LABEL_AND_DESCRIPTION | l2           | True  | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | False | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| LABEL_AND_DESCRIPTION | ip           | True  | 1  | 0.3727 | 0.3727    | 0.3727  |\n",
    "| ALL_OCCUPATIONS     | cosine         | False | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| ALL_OCCUPATIONS     | cosine         | True  | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| ALL_OCCUPATIONS     | l2             | False | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| ALL_OCCUPATIONS     | l2             | True  | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| ALL_OCCUPATIONS     | ip             | False | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| ALL_OCCUPATIONS     | ip             | True  | 1  | 0.3469 | 0.3469    | 0.3469  |\n",
    "| DESCRIPTION         | cosine         | False | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "| DESCRIPTION         | cosine         | True  | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "| DESCRIPTION         | l2             | False | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "| DESCRIPTION         | l2             | True  | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "| DESCRIPTION         | ip             | False | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "| DESCRIPTION         | ip             | True  | 1  | 0.3395 | 0.3395    | 0.3395  |\n",
    "\n",
    "\n",
    "The result of the evaluation are as follows:\n",
    "\n",
    "1. The results obtained without MMR are definitely better than the results obtained without MMR. This happens because the correct code is most of the times within the first k elements and still very similar to the first one. MMR excludes many good high ranking results that could be retrieved otherwise because they are too similar to the first result.\n",
    "2. Different retrieval functions return the same results. This probably happens because the Gecko embeddings are normalized so that l2, cosine similarity and dot product all determine the same ranking. We will choose the default version.\n",
    "3. The best embedding methods is PREFERREDLABEL, slightly higher than ALL_OCCUPATIONS in our k of interest (3 or 5). We are choosing PREFERREDLABEL given that it has a small margin over ALL_OCCUPATIONS and it's easier to implement as its value is already present in the dataset. We think that the information contained in such label helps the model identify the correct occupation even when they are not using the same words, as the correspondence between ESCO nodes and PREFERREDLABEL is one-to-one. On the other hand, a secondary labels can appear in multiple nodes, thus confusing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills\n",
    "\n",
    "| method            | score function | MMR   | k   | recall | precision | f-score |\n",
    "|-------------------|----------------|-------|-----|--------|-----------|---------|\n",
    "| PREFERREDLABEL        | cosine           | False |  10 |   0.5446 |      0.0877 |    0.1511 |\n",
    "| PREFERREDLABEL        | l2               | False |  10 |   0.5446 |      0.0877 |    0.1511 |\n",
    "| PREFERREDLABEL        | ip               | False |  10 |   0.5446 |      0.0877 |    0.1511 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | False |  10 |   0.5029 |      0.0793 |    0.137  |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | False |  10 |   0.5027 |      0.0792 |    0.1369 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | False |  10 |   0.5027 |      0.0792 |    0.1369 |\n",
    "| ALL_SKILLS            | cosine           | False |  10 |   0.4965 |      0.0777 |    0.1344 |\n",
    "| ALL_SKILLS            | ip               | False |  10 |   0.4965 |      0.0777 |    0.1344 |\n",
    "| ALL_SKILLS            | l2               | False |  10 |   0.4964 |      0.0776 |    0.1342 |\n",
    "| DESCRIPTION           | cosine           | False |  10 |   0.3917 |      0.0604 |    0.1047 |\n",
    "| DESCRIPTION           | l2               | False |  10 |   0.3917 |      0.0604 |    0.1047 |\n",
    "| DESCRIPTION           | ip               | False |  10 |   0.3917 |      0.0604 |    0.1047 |\n",
    "| PREFERREDLABEL        | l2               | True  |  10 |   0.349  |      0.0553 |    0.0955 |\n",
    "| PREFERREDLABEL        | ip               | True  |  10 |   0.349  |      0.0553 |    0.0955 |\n",
    "| PREFERREDLABEL        | cosine           | True  |  10 |   0.3486 |      0.0552 |    0.0953 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | True  |  10 |   0.3111 |      0.0482 |    0.0835 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | True  |  10 |   0.3109 |      0.0481 |    0.0834 |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | True  |  10 |   0.3106 |      0.048  |    0.0832 |\n",
    "| ALL_SKILLS            | cosine           | True  |  10 |   0.299  |      0.0463 |    0.0802 |\n",
    "| ALL_SKILLS            | ip               | True  |  10 |   0.299  |      0.0463 |    0.0802 |\n",
    "| ALL_SKILLS            | l2               | True  |  10 |   0.2989 |      0.0462 |    0.0801 |\n",
    "| DESCRIPTION           | cosine           | True  |  10 |   0.226  |      0.0337 |    0.0587 |\n",
    "| DESCRIPTION           | l2               | True  |  10 |   0.2258 |      0.0337 |    0.0586 |\n",
    "| DESCRIPTION           | ip               | True  |  10 |   0.2258 |      0.0337 |    0.0586 |\n",
    "| PREFERREDLABEL        | cosine           | False |   5 |   0.4637 |      0.1451 |    0.221  |\n",
    "| PREFERREDLABEL        | l2               | False |   5 |   0.4637 |      0.1451 |    0.221  |\n",
    "| PREFERREDLABEL        | ip               | False |   5 |   0.4637 |      0.1451 |    0.221  |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | False |   5 |   0.4159 |      0.1264 |    0.1939 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | False |   5 |   0.4159 |      0.1264 |    0.1939 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | False |   5 |   0.4159 |      0.1264 |    0.1939 |\n",
    "| ALL_SKILLS            | cosine           | False |   5 |   0.4145 |      0.1272 |    0.1946 |\n",
    "| ALL_SKILLS            | ip               | False |   5 |   0.4145 |      0.1272 |    0.1946 |\n",
    "| ALL_SKILLS            | l2               | False |   5 |   0.4144 |      0.127  |    0.1944 |\n",
    "| PREFERREDLABEL        | l2               | True  |   5 |   0.3385 |      0.106  |    0.1615 |\n",
    "| PREFERREDLABEL        | ip               | True  |   5 |   0.3385 |      0.106  |    0.1615 |\n",
    "| PREFERREDLABEL        | cosine           | True  |   5 |   0.338  |      0.1058 |    0.1612 |\n",
    "| DESCRIPTION           | cosine           | False |   5 |   0.3045 |      0.0921 |    0.1414 |\n",
    "| DESCRIPTION           | l2               | False |   5 |   0.3045 |      0.0921 |    0.1414 |\n",
    "| DESCRIPTION           | ip               | False |   5 |   0.3045 |      0.0921 |    0.1414 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | True  |   5 |   0.2979 |      0.0904 |    0.1387 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | True  |   5 |   0.2978 |      0.0904 |    0.1387 |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | True  |   5 |   0.2976 |      0.0902 |    0.1384 |\n",
    "| ALL_SKILLS            | cosine           | True  |   5 |   0.2913 |      0.0892 |    0.1366 |\n",
    "| ALL_SKILLS            | ip               | True  |   5 |   0.2913 |      0.0892 |    0.1366 |\n",
    "| ALL_SKILLS            | l2               | True  |   5 |   0.2911 |      0.089  |    0.1364 |\n",
    "| DESCRIPTION           | cosine           | True  |   5 |   0.2178 |      0.0641 |    0.099  |\n",
    "| DESCRIPTION           | l2               | True  |   5 |   0.2178 |      0.0641 |    0.099  |\n",
    "| DESCRIPTION           | ip               | True  |   5 |   0.2178 |      0.0641 |    0.099  |\n",
    "| PREFERREDLABEL        | cosine           | False |   3 |   0.3982 |      0.2018 |    0.2678 |\n",
    "| PREFERREDLABEL        | l2               | False |   3 |   0.3982 |      0.2018 |    0.2678 |\n",
    "| PREFERREDLABEL        | ip               | False |   3 |   0.3982 |      0.2018 |    0.2678 |\n",
    "| ALL_SKILLS            | cosine           | False |   3 |   0.3563 |      0.1779 |    0.2374 |\n",
    "| ALL_SKILLS            | ip               | False |   3 |   0.3563 |      0.1779 |    0.2374 |\n",
    "| ALL_SKILLS            | l2               | False |   3 |   0.3561 |      0.1776 |    0.237  |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | False |   3 |   0.3535 |      0.176  |    0.235  |\n",
    "| LABEL_AND_DESCRIPTION | l2               | False |   3 |   0.3535 |      0.176  |    0.235  |\n",
    "| LABEL_AND_DESCRIPTION | ip               | False |   3 |   0.3535 |      0.176  |    0.235  |\n",
    "| PREFERREDLABEL        | l2               | True  |   3 |   0.3278 |      0.1694 |    0.2233 |\n",
    "| PREFERREDLABEL        | ip               | True  |   3 |   0.3278 |      0.1694 |    0.2233 |\n",
    "| PREFERREDLABEL        | cosine           | True  |   3 |   0.3273 |      0.169  |    0.223  |\n",
    "| LABEL_AND_DESCRIPTION | l2               | True  |   3 |   0.2826 |      0.1401 |    0.1874 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | True  |   3 |   0.2826 |      0.1401 |    0.1874 |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | True  |   3 |   0.2823 |      0.1398 |    0.187  |\n",
    "| ALL_SKILLS            | cosine           | True  |   3 |   0.2751 |      0.1389 |    0.1846 |\n",
    "| ALL_SKILLS            | ip               | True  |   3 |   0.2751 |      0.1389 |    0.1846 |\n",
    "| ALL_SKILLS            | l2               | True  |   3 |   0.2749 |      0.1385 |    0.1842 |\n",
    "| DESCRIPTION           | cosine           | False |   3 |   0.254  |      0.1258 |    0.1683 |\n",
    "| DESCRIPTION           | l2               | False |   3 |   0.254  |      0.1258 |    0.1683 |\n",
    "| DESCRIPTION           | ip               | False |   3 |   0.254  |      0.1258 |    0.1683 |\n",
    "| DESCRIPTION           | cosine           | True  |   3 |   0.2048 |      0.0979 |    0.1324 |\n",
    "| DESCRIPTION           | l2               | True  |   3 |   0.2048 |      0.0979 |    0.1324 |\n",
    "| DESCRIPTION           | ip               | True  |   3 |   0.2048 |      0.0979 |    0.1324 |\n",
    "| PREFERREDLABEL        | cosine           | True  |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| PREFERREDLABEL        | l2               | True  |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| PREFERREDLABEL        | ip               | True  |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| PREFERREDLABEL        | cosine           | False |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| PREFERREDLABEL        | l2               | False |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| PREFERREDLABEL        | ip               | False |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | True  |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | True  |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | True  |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| LABEL_AND_DESCRIPTION | cosine           | False |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| LABEL_AND_DESCRIPTION | l2               | False |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| LABEL_AND_DESCRIPTION | ip               | False |   1 |   0.2142 |      0.3012 |    0.2504 |\n",
    "| ALL_SKILLS            | cosine           | True  |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| ALL_SKILLS            | l2               | True  |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| ALL_SKILLS            | ip               | True  |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| ALL_SKILLS            | cosine           | False |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| ALL_SKILLS            | l2               | False |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| ALL_SKILLS            | ip               | False |   1 |   0.1984 |      0.286  |    0.2343 |\n",
    "| DESCRIPTION           | cosine           | True  |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "| DESCRIPTION           | l2               | True  |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "| DESCRIPTION           | ip               | True  |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "| DESCRIPTION           | cosine           | False |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "| DESCRIPTION           | l2               | False |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "| DESCRIPTION           | ip               | False |   1 |   0.1531 |      0.2126 |    0.178  |\n",
    "\n",
    "The result of the evaluation are similar to those for the occupations, that is:\n",
    "\n",
    "1. The results obtained without MMR are definitely better than the results obtained without MMR. This happens because the correct code is most of the times within the first k elements and still very similar to the first one. MMR excludes many good high ranking results that could be retrieved otherwise because they are too similar to the first result.\n",
    "2. Different retrieval functions return the same results. This probably happens because the Gecko embeddings are normalized so that l2, cosine similarity and dot product all determine the same ranking. We will choose the default version.\n",
    "3. The best embedding methods is PREFERREDLABEL, higher than ALL_SKILLS in our k of interest (3 or 5). PREFERREDLABEL has a high margin over the second best and it's easier to implement as its value is already present in the dataset. We think that the information contained in such label helps the model identify the correct skill even when they are not using the same words, as the correspondence between ESCO nodes and PREFERREDLABEL is one-to-one. On the other hand, a secondary labels can appear in multiple nodes, thus confusing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, for both skills and occupations we choose the hyperparameters to be without MMR, using the node embedding corresponding to preferred label and we choose the default retrieval function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linking skills to occupation statements\n",
    "\n",
    "Another line of research involves understanding how to best link skills to a statement about the user's occupation. In practice, when we're only interested in understanding which skills are pertinent to the user, should we link the occupation statement directly to the skills dataset or should we focus on finding the right occupation and retrieving the related skills directly from the ESCO model?\n",
    "\n",
    "In the evaluation that follows, we use the Occupation dataset having as ground truth the essential skills related to each occupation and we compare the two methods. The first method is as follows:\n",
    "\n",
    "1. We fix the method and score functions as the optimal parameters for the previous search and load the corresponding **skills** collection.\n",
    "2. For each element in the **occupation** test set, we find the top 100 **skills** in the collection ordered by scoring rank.\n",
    "3. We filter those **skills** by maximal marginal relevance to find the top 10 **skills** with this function.\n",
    "4. We evaluate the precision, recall and F-score on the top k for k=1,3,5,10 for the retrieved **skills**, using as ground truth the **essential skills** for the original **occupation**.\n",
    "5. We evaluate the precision, recall and F-score on the top k for k=1,3,5,10 for the **skills** filtered by maximal marginal relevance, using as ground truth the **essential skills** for the original occupation.\n",
    "6. We save the results in a dataframe to be analyzed.\n",
    "\n",
    "On the other hand, the second method works as follows:\n",
    "\n",
    "1. We fix the method and score functions as the optimal parameters for the previous search and load the corresponding **occupation** collection.\n",
    "2. For each element in the test set, we find the top 100 **occupation** documents in the collection ordered by scoring rank.\n",
    "3. We filter those documents by maximal marginal relevance to find the top 10 **occupations** with this function.\n",
    "4. For each element of the test set, we consider the true value to be the set of all **essential skills** related to the occupation.\n",
    "5. For each k=1,3,5,10, we consider as predicted elements the set of all **essential skills** related to the top k retrieved occupations (either in regular or MMR vector search).\n",
    "5. We evaluate the precision, recall and F-score on the retrieved skills, both with regular and MMR vector search.\n",
    "6. We save the results in a dataframe to be analyzed for later use.\n",
    "\n",
    "Notice that the second method is expected to return a recall that is strictly better than the occupation search evaluation, as a correct occupation implies that all the necessary skills are retrieved as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset that maps occupation ESCO IDs to the corresponding Tabiya UUID of essential skills\n",
    "# This allows us to evaluate how to retrieve skills from linking to occupations\n",
    "occupation_id_to_esco_code = {row[\"ID\"]:row[\"CODE\"] for _, row in df_occupation_database.iterrows()}\n",
    "skill_id_to_uuid = {row[\"ID\"]: row[\"CODE\"] for _, row in df_skill_database.iterrows()}\n",
    "grouped_df = df_occupation_to_skills.groupby([\"OCCUPATIONID\",\"RELATIONTYPE\"])[\"SKILLID\"].agg(list).reset_index()\n",
    "esco_id_to_skills_essential = {occupation_id_to_esco_code[row[\"OCCUPATIONID\"]]:[skill_id_to_uuid[skill_id] for skill_id in row[\"SKILLID\"]] for _, row in grouped_df.iterrows() if row[\"RELATIONTYPE\"]==\"essential\"}\n",
    "for occ_id, esco_code in occupation_id_to_esco_code.items():\n",
    "    if esco_code not in esco_id_to_skills_essential:\n",
    "        esco_id_to_skills_essential[esco_code] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def run_skill_occupation_eval(method_list: List[str], score_function_list: List[str], df_test: pd.DataFrame, esco_id_to_skills_essential: Dict[str,List[str]], embedding_column: str = \"embeddings\") -> pd.DataFrame:\n",
    "    \"\"\"Returns the results of an evaluation for occupation-related skills\n",
    "    on a list of collections\n",
    "\n",
    "    Args:\n",
    "        method_list (List[str]): list of methods of interest\n",
    "            to test.\n",
    "        score_function_list (List[str]): list of score functions of interest\n",
    "            to test.\n",
    "        df_test (pd.DataFrame): test dataframe, containing an embedding column\n",
    "            and a test_target column.\n",
    "        esco_id_to_skills_essential (Dict[str,List[str]]): dictionary mapping each\n",
    "            occupation ESCO id to a list of essential skills Tabiya UUID.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the result of the evaluation depending on the\n",
    "            different hyperparameters.\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    for method in method_list:\n",
    "        for score in score_function_list:\n",
    "            collection_name = f\"occupations_{method}_{score}\"\n",
    "            # Fetch collection\n",
    "            collection = client.get_collection(name=collection_name)\n",
    "            # Initialize lists to save results\n",
    "            vector_search_results, mmr_vector_search_results = get_results_from_embeddings(list(df_test[embedding_column]), collection)\n",
    "            # Evaluate accuracy at k for k=1, 3, 5, 10\n",
    "            for k in [1, 3, 5, 10]:\n",
    "                # Link the retrieved ESCO codes to their essential skills\n",
    "                skill_related_vs_results = [set(sum([esco_id_to_skills_essential[code] for code in elem[:k]], start=[])) for elem in vector_search_results]\n",
    "                skill_related_mmr_vs_results = [set(sum([esco_id_to_skills_essential[code] for code in elem[:k]], start=[])) for elem in mmr_vector_search_results]\n",
    "                # Finds precision, recall and F-score for the skills retrieved from the top k occupations\n",
    "                rec_at_k, prec_at_k, f_score_at_k = get_all_metrics(skill_related_vs_results, list(df_test[\"skills_essential\"]))\n",
    "                eval_data.append({\"method\":method, \"score function\":score, \"MMR\": False, \"k\":k, \"recall\": round(rec_at_k, 4), \"precision\": round(prec_at_k,4), \"f-score\": round(f_score_at_k,4)})\n",
    "                rec_at_k, prec_at_k, f_score_at_k = get_all_metrics(skill_related_mmr_vs_results, list(df_test[\"skills_essential\"]))\n",
    "                eval_data.append({\"method\":method, \"score function\":score, \"MMR\": True, \"k\":k, \"recall\": round(rec_at_k, 4), \"precision\": round(prec_at_k,4), \"f-score\": round(f_score_at_k,4)})\n",
    "    # Save the results in a dataframe\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of skill linking to occupation. Modify the notebook to save the data locally.\n",
    "df_skill_occupation_eval1 = run_eval_for_multiple_collections(\"skills\", [\"PREFERREDLABEL\"], [\"cosine\"], df_occupation_test, target_column=\"skills_essential\")\n",
    "df_skill_occupation_eval2 = run_skill_occupation_eval([\"PREFERREDLABEL\"], [\"cosine\"], df_occupation_test, esco_id_to_skills_essential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation-related Skills\n",
    "\n",
    "The first method has the following results:\n",
    "\n",
    "| method        | k   | recall | precision | f-score |\n",
    "|---------------|-----|--------|-----------|---------|\n",
    "| PREFERREDLABEL| 10  | 0.0571 | 0.1413    | 0.0813  |\n",
    "| PREFERREDLABEL| 5   | 0.0334 | 0.1631    | 0.0554  |\n",
    "| PREFERREDLABEL| 3   | 0.0209 | 0.1697    | 0.0372  |\n",
    "| PREFERREDLABEL| 1   | 0.0085 | 0.1845    | 0.0162  |\n",
    "\n",
    "While the second method returns the following scores:\n",
    "\n",
    "| method        | k   | recall | precision | f-score |\n",
    "|---------------|-----|--------|-----------|---------|\n",
    "| PREFERREDLABEL| 10  | 0.8559 | 0.1614    | 0.2716  |\n",
    "| PREFERREDLABEL| 5   | 0.7772 | 0.2471    | 0.375   |\n",
    "| PREFERREDLABEL| 3   | 0.7042 | 0.326     | 0.4457  |\n",
    "| PREFERREDLABEL| 1   | 0.4982 | 0.5002    | 0.4992  |\n",
    "\n",
    "We clearly retrieve much better results in the second case. In fact, there is an average of 27 essential skills for each occupation and the first experiment only retrieves a small number of them for k=1,3,5,10. Rather than retrieving large number of skills, we instead retrieve a small number of occupations in the second experiment and link them to their essential skills using the ESCO database. In this case, the recall results are much higher than in occupation linking. This is expected, since correct occupations imply correct related essential skills. However, the precision tends to decrease fast, as the number of skills increases by an average of 27k. Therefore, we suggest to keep a low value of k if we want to apply a similar process for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Are job title queries better indicators than their descriptions?\n",
    "\n",
    "In this last experiment, we want to understand whether, from the point of view of the user, returning exclusively the correct job title when asked about the past experiences is more convenient than giving a full description of the occupation that they had. The queries we generated synthetically return a randomized selection based on submitted title and job descriptions, so that not all the answers to the query highlight the specific job title of interest. We proceed to the evaluation in the following way:\n",
    "\n",
    "1. We consider whether having the job title as query performs better than not declaring it explicitly.\n",
    "2. We consider the subset of the test set in which the job title doesn't perform well and we evaluate if other methods that mention the description in the target node perform better than PREFERREDLABEL.\n",
    "\n",
    "In the first case, we compare the best result for each k to the corresponding best results from the previous evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the title queries in the occupation test set\n",
    "title_test_occupation_embeddings = embed_strings_in_batch(list(df_occupation_test[\"title\"]), model)\n",
    "df_occupation_test[\"title_embeddings\"] = title_test_occupation_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of occupation. Modify the notebook to save locally.\n",
    "df_occupation_eval = run_eval_for_multiple_collections(\"occupations\", method_list =list(function_to_occupation_method.keys()), score_function_list=[\"cosine\"], df_test=df_occupation_test, embedding_column=\"title_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupation search\n",
    "\n",
    "| Method              | k   | Recall | Precision | F-score | Input Type                |\n",
    "|---------------------|-----|--------|-----------|---------|---------------------------|\n",
    "| ALL_OCCUPATIONS     | 10  | 0.7491 | 0.0749    | 0.1362  | Title                     |\n",
    "| ALL_OCCUPATIONS     | 10  | 0.738  | 0.0738    | 0.1342  | Synthetic Query           |\n",
    "| PREFERREDLABEL      | 10  | 0.7362 | 0.0736    | 0.1338  | Title                     |\n",
    "| PREFERREDLABEL      | 10  | 0.7454 | 0.0745    | 0.1355  | Synthetic Query           |\n",
    "| DESCRIPTION         | 10  | 0.7196 | 0.072     | 0.1308  | Title                     |\n",
    "| DESCRIPTION         | 10  | 0.7122 | 0.0712    | 0.1295  | Synthetic Query           |\n",
    "| LABEL_AND_DESCRIPTION | 10 | 0.6956 | 0.0696    | 0.1265  | Title                     |\n",
    "| LABEL_AND_DESCRIPTION | 10 | 0.7196 | 0.072     | 0.1308  | Synthetic Query           |\n",
    "| ALL_OCCUPATIONS     | 5   | 0.6882 | 0.1376    | 0.2294  | Title                     |\n",
    "| ALL_OCCUPATIONS     | 5   | 0.6605 | 0.1321    | 0.2202  | Synthetic Query           |\n",
    "| PREFERREDLABEL      | 5   | 0.6716 | 0.1343    | 0.2239  | Title                     |\n",
    "| PREFERREDLABEL      | 5   | 0.6624 | 0.1325    | 0.2208  | Synthetic Query           |\n",
    "| DESCRIPTION         | 5   | 0.6458 | 0.1292    | 0.2153  | Title                     |\n",
    "| DESCRIPTION         | 5   | 0.6181 | 0.1236    | 0.206   | Synthetic Query           |\n",
    "| LABEL_AND_DESCRIPTION | 5 | 0.6347 | 0.1269    | 0.2116  | Title                     |\n",
    "| LABEL_AND_DESCRIPTION | 5 | 0.6144 | 0.1229    | 0.2048  | Synthetic Query           |\n",
    "| PREFERREDLABEL      | 3   | 0.6236 | 0.2079    | 0.3118  | Title                     |\n",
    "| PREFERREDLABEL      | 3   | 0.5812 | 0.1937    | 0.2906  | Synthetic Query           |\n",
    "| ALL_OCCUPATIONS     | 3   | 0.6162 | 0.2054    | 0.3081  | Title                     |\n",
    "| ALL_OCCUPATIONS     | 3   | 0.5812 | 0.1937    | 0.2906  | Synthetic Query           |\n",
    "| DESCRIPTION         | 3   | 0.5775 | 0.1925    | 0.2887  | Title                     |\n",
    "| DESCRIPTION         | 3   | 0.5424 | 0.1808    | 0.2712  | Synthetic Query           |\n",
    "| LABEL_AND_DESCRIPTION | 3 | 0.559  | 0.1863    | 0.2795  | Title                     |\n",
    "| LABEL_AND_DESCRIPTION | 3 | 0.548  | 0.1827    | 0.274   | Synthetic Query           |\n",
    "| PREFERREDLABEL      | 1   | 0.4354 | 0.4354    | 0.4354  | Title                     |\n",
    "| PREFERREDLABEL      | 1   | 0.3801 | 0.3801    | 0.3801  | Synthetic Query           |\n",
    "| ALL_OCCUPATIONS     | 1   | 0.4262 | 0.4262    | 0.4262  | Title                     |\n",
    "| ALL_OCCUPATIONS     | 1   | 0.3469 | 0.3469    | 0.3469  | Synthetic Query           |\n",
    "| LABEL_AND_DESCRIPTION | 1 | 0.4133 | 0.4133    | 0.4133  | Title                     |\n",
    "| LABEL_AND_DESCRIPTION | 1 | 0.3727 | 0.3727    | 0.3727  | Synthetic Query           |\n",
    "| DESCRIPTION         | 1   | 0.393  | 0.393     | 0.393   | Title                     |\n",
    "| DESCRIPTION         | 1   | 0.3395 | 0.3395    | 0.3395  | Synthetic Query           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing these results to those in the first evaluation, we notice a couple of things:\n",
    "1. For lower values of k using the title as query with the **preferred label** as target returns a higher recall than using the synthetic query. For the preferred label target, this advantage tends to reduce as k grows and is actually negative for k=10. This happens because the synthetic query might contain more information that can be linked to the preferred label when the title is not found within the first 3 or 5.\n",
    "2. For higher values of k, using the title as query with the **all occupations** method, which includes secondary labels and description performs better than using the synthetic query. This happens because when the title is not identified with the preferred label, there is a chance it could match a secondary label that is present in the all occupations method. \n",
    "3. The overall title performance is better than the synthetic query, although this gap is smaller for k=10. This implies that there is an inherent advantage in using models of Named Entity Recognition or asking the user directly for the title of his previous jobs.\n",
    "\n",
    "We now ask what is the nature of those datapoints whose elements are not retrieved through the job title when k=1. Can they be identified with other fields of the collection? For that reason, we run an evaluation both on their title and on their synthetic query and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the results of linking the title to the preferred label and consider the filtered dataframe in which \n",
    "# this result doesn't coincide with the ground truth\n",
    "label_collection = client.get_collection(\"occupations_PREFERREDLABEL_cosine\")\n",
    "df_occupation_test[\"title_results_preferredlabel\"] = df_occupation_test[\"title_embeddings\"].apply(lambda x: get_results_from_embeddings([x], label_collection)[0][0][0])\n",
    "\n",
    "filtered_df = df_occupation_test[(df_occupation_test[\"esco_code\"]!=df_occupation_test[\"title_results_preferredlabel\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation. Modify the notebook to save the results locally.\n",
    "filtered_df_eval = run_eval_for_multiple_collections(\"occupations\", method_list =list(function_to_occupation_method.keys()), score_function_list=[\"cosine\"], df_test=filtered_df, embedding_column=\"embeddings\")\n",
    "filtered_df_title_eval = run_eval_for_multiple_collections(\"occupations\", method_list =[\"DESCRIPTION\", \"ALL_OCCUPATIONS\", \"LABEL_AND_DESCRIPTION\"], score_function_list=[\"cosine\"], df_test=filtered_df, embedding_column=\"title_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict the evaluation results to the case k=1 and observe how much additional information can be obtained from the other fields of the collection.\n",
    "\n",
    "### Title linking to fields\n",
    "\n",
    "| method               | score function | k   | recall | precision | f-score |\n",
    "|----------------------|----------------|-----|--------|-----------|---------|\n",
    "| ALL_OCCUPATIONS     | cosine         | 1   | 0.1438 | 0.1438    | 0.1438  |\n",
    "| DESCRIPTION          | cosine         | 1   | 0.1046 | 0.1046    | 0.1046  |\n",
    "| LABEL_AND_DESCRIPTION| cosine         | 1   | 0.098  | 0.098     | 0.098   |\n",
    "\n",
    "### Synthetic query linking to fields\n",
    "\n",
    "| method               | score function | k   | recall | precision | f-score |\n",
    "|----------------------|----------------|-----|--------|-----------|---------|\n",
    "| ALL_OCCUPATIONS     | cosine         | 1   | 0.1895 | 0.1895    | 0.1895  |\n",
    "| LABEL_AND_DESCRIPTION| cosine         | 1   | 0.1895 | 0.1895    | 0.1895  |\n",
    "| DESCRIPTION          | cosine         | 1   | 0.183  | 0.183     | 0.183   |\n",
    "| PREFERREDLABEL       | cosine         | 1   | 0.1699 | 0.1699    | 0.1699  |\n",
    "\n",
    "Interestingly the largest recall boost is given by linking to the ALL_OCCUPATIONS field combination, which is the only one that contains secondary labels. In case our query coincides with the title, this is probably the ground for the 5% recall boost when compared to the other fields. In the case of the synthetic query, however, we see how this is pretty similar to the description, so that most likely we gain from the similarity between the description of the experience and the description field in the node. \n",
    "\n",
    "We suggest that if an NER method were to be implemented before the linking, we could experiment a composite method which links the NER-retrieved span in a sentence to the PREFERREDLABEL, but any whole sentence with no retrieved span to the ALL_OCCUPATIONS field. This is not guaranteed to benefit from the gains of this past analysis, as it is strongly dependent on the NER performance, but might benefit from it if we knew that the job titles for which we are confident in the NER are also those that can be easily linked to ESCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Does segmentation through fields improve recall on Occupation?\n",
    "\n",
    "In our previous experiment we wanted to find the best method to link the occupations of the test set to the ESCO database. We allowed each ESCO node to have a single embedding that could only be a combination of the strings in its fields (preferred label, secondary labels and description). We found that for low values of k, the preferred label guaranteed a higher recall, while higher values of k a combination of all fields guaranteed a better outcome.\n",
    "\n",
    "We now want to consider the possibility that each node can be represented by multiple embeddings. This is similar to the way in which documents can be segmented and embedded in information retrieval. We consider two approaches:\n",
    "\n",
    "1. We embed each document with three embeddings: preferred label, secondary labels and description. \n",
    "2. We embed each document with more than three embeddings, including one embedding for each secondary label.\n",
    "\n",
    "We first generate the results and then compare their recall to the results of the previous evaluation. We compare the results for both synthetic queries and titles.\n",
    "\n",
    "Notice that in our retrieval function, we consider k to be the set of unique top esco codes within the first 100 entries, meaning that the more embeddings per node we load in the collection, the least amount of different nodes we will find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and create the collection on three embeddings\n",
    "\n",
    "def create_collection_three_embeddings(df_database, collection_type):\n",
    "    df_full_list = []\n",
    "    for _, row in tqdm(df_database.iterrows()):\n",
    "        for field in [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\"]:\n",
    "            df_full_list.append({\"text\": str(row[field]), \"CODE\":row[\"CODE\"]})\n",
    "    full_df = pd.DataFrame(df_full_list)\n",
    "    full_df[\"embeddings\"] = embed_strings_in_batch(list(full_df[\"text\"]), model)\n",
    "    create_collection_in_batch(f\"{collection_type}_three_embeddings\", full_df)\n",
    "\n",
    "create_collection_three_embeddings(df_occupation_database, \"occupation\")\n",
    "create_collection_three_embeddings(df_skill_database, \"skill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and create the collection for multiple embeddings\n",
    "\n",
    "def create_collection_multiple_embeddings(df_database, collection_type):\n",
    "    df_full_list = []\n",
    "    for _, row in tqdm(df_database.iterrows()):\n",
    "        for field in [\"PREFERREDLABEL\", \"DESCRIPTION\"]:\n",
    "            df_full_list.append({\"text\": row[field], \"CODE\":row[\"CODE\"]})\n",
    "        for sec_label in str(row[\"ALTLABELS\"]).split(\"\\n\"):\n",
    "            df_full_list.append({\"text\": sec_label, \"CODE\":row[\"CODE\"]})\n",
    "    full_df = pd.DataFrame(df_full_list)\n",
    "    full_df[\"embeddings\"] = embed_strings_in_batch(list(full_df[\"text\"]), model)\n",
    "    create_collection_in_batch(f\"{collection_type}_multiple_embeddings\", full_df)\n",
    "\n",
    "create_collection_multiple_embeddings(df_occupation_database, \"occupation\")\n",
    "create_collection_multiple_embeddings(df_skill_database, \"skill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the specific application\n",
    "\n",
    "def run_eval_multiple_embeddings(collection_name: str, df_test: pd.DataFrame, target_column: str = \"CODE\", embedding_column = \"embeddings\") -> pd.DataFrame:\n",
    "    \"\"\"Returns the results of an evaluation on a list of collections\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): name of the collection.\n",
    "        df_test (pd.DataFrame): test dataframe, containing an embedding column\n",
    "            and a test_target column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the result of the evaluation depending on the\n",
    "            different hyperparameters.\n",
    "    \"\"\"\n",
    "    eval_data = []\n",
    "    # Fetch collection\n",
    "    collection = client.get_collection(collection_name)\n",
    "    # Initialize lists to save results\n",
    "    vector_search_results = get_top_n_results_from_embeddings(list(df_test[embedding_column]), collection)\n",
    "    # Find single esco codes eliminating duplicates\n",
    "    single_vs_results = [list(set(elem)) for elem in vector_search_results]\n",
    "    vector_search_results_single = [sorted(elem, key=vs_elem.index) for elem, vs_elem in zip(single_vs_results, vector_search_results)]\n",
    "    # Evaluate accuracy at k for k=1, 3, 5, 10\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        rec_at_k, prec_at_k, f_score_at_k = get_all_metrics(vector_search_results_single, list(df_test[target_column]), k)\n",
    "        eval_data.append({\"method\": collection_name, \"embedded field\": \"title\" if embedding_column==\"title_embeddings\" else \"synthetic query\", \"k\":k, \"recall\": round(rec_at_k, 4), \"precision\": round(prec_at_k,4), \"f-score\": round(f_score_at_k,4)})\n",
    "# Save the results in a dataframe\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full evaluation for titles and synthetic queries. Modify the notebook to save the results locally.\n",
    "occ_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_multiple_embeddings\", \"occupation_three_embeddings\"]:\n",
    "    occ_eval_df = pd.concat([occ_eval_df, run_eval_multiple_embeddings(collection_name, df_occupation_test)])\n",
    "    occ_eval_df = pd.concat([occ_eval_df, run_eval_multiple_embeddings(collection_name, df_occupation_test, embedding_column=\"title_embeddings\")])\n",
    "\n",
    "# Run the full evaluation for synthetic queries on skills. Modify the notebook to save the results locally.\n",
    "sk_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"skill_multiple_embeddings\", \"skill_three_embeddings\"]:\n",
    "    sk_eval_df = pd.concat([sk_eval_df, run_eval_multiple_embeddings(collection_name, df_skill_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows, also including those from the previous evaluation with single field. For occupations:\n",
    "\n",
    "| Method                          | k  | Recall | Precision | F-score | Input Type       |\n",
    "|---------------------------------|----|--------|-----------|---------|------------------|\n",
    "| occupation_multiple_embeddings | 10 | 0.7657 | 0.0766    | 0.1392  | title            |\n",
    "| occupation_three_embeddings    | 10 | 0.7601 | 0.076     | 0.1382  | synthetic query  |\n",
    "| occupation_three_embeddings    | 10 | 0.7546 | 0.0755    | 0.1372  | title            |\n",
    "| ALL_OCCUPATIONS                 | 10 | 0.7491 | 0.0749    | 0.1362  | title            |\n",
    "| occupation_multiple_embeddings | 10 | 0.7435 | 0.0744    | 0.1352  | synthetic query  |\n",
    "| ALL_OCCUPATIONS                 | 10 | 0.738  | 0.0738    | 0.1342  | synthetic query |\n",
    "| occupation_three_embeddings    | 5  | 0.6919 | 0.1384    | 0.2306  | title            |\n",
    "| occupation_three_embeddings    | 5  | 0.6919 | 0.1384    | 0.2306  | synthetic query  |\n",
    "| ALL_OCCUPATIONS                 | 5  | 0.6882 | 0.1376    | 0.2294  | title            |\n",
    "| ALL_OCCUPATIONS                 | 5  | 0.6605 | 0.1321    | 0.2202  | synthetic query  |\n",
    "| occupation_multiple_embeddings | 5  | 0.6568 | 0.1314    | 0.2189  | title            |\n",
    "| occupation_multiple_embeddings | 5  | 0.6494 | 0.1299    | 0.2165  | synthetic query  |\n",
    "| occupation_three_embeddings    | 3  | 0.6421 | 0.214     | 0.321   | title            |\n",
    "| ALL_OCCUPATIONS                 | 3  | 0.6162 | 0.2054    | 0.3081  | title            |\n",
    "| occupation_three_embeddings    | 3  | 0.5959 | 0.1986    | 0.298   | synthetic query  |\n",
    "| occupation_multiple_embeddings | 3  | 0.583  | 0.1943    | 0.2915  | title            |\n",
    "| ALL_OCCUPATIONS                 | 3  | 0.5812 | 0.1937    | 0.2906  | synthetic query  |\n",
    "| occupation_multiple_embeddings | 3  | 0.5646 | 0.1882    | 0.2823  | synthetic query  |\n",
    "| occupation_three_embeddings    | 1  | 0.441  | 0.441     | 0.441   | title            |\n",
    "| ALL_OCCUPATIONS                 | 1  | 0.4262 | 0.4262    | 0.4262  | title            |\n",
    "| occupation_three_embeddings    | 1  | 0.3819 | 0.3819    | 0.3819  | synthetic query  |\n",
    "| occupation_multiple_embeddings | 1  | 0.3579 | 0.3579    | 0.3579  | synthetic query  |\n",
    "| occupation_multiple_embeddings | 1  | 0.3561 | 0.3561    | 0.3561  | title            |\n",
    "| ALL_OCCUPATIONS                 | 1  | 0.3469 | 0.3469    | 0.3469  | synthetic query  |\n",
    "\n",
    "We can notice the following trends:\n",
    "\n",
    "- When using the **synthetic query**, embedding preferred labels, secondary labels and description separately is stably more effective than embedding them all together or than separating all the possible secondary labels. This makes sense, as we get some advantages over the multiple secondary labels embeddings by avoiding flooding the database with the same embedding for multiple labels. This is also stably better than the combination of all fields, probably because the embeddings are more focused and the more descriptive queries can be matched to the description directly, while those that are focused on the title can be matched to either preferred or secondary labels.\n",
    "- When using the **title**, the method with three embeddings is more effective than all the others for low values of k. For higher values of k this doesn't hold anymore and the method with multiple embeddings has some gains as the titles are probably closer to each single preferred label. Since we don't care that most of the labels are not correct, this guarantees an advantage over the flooding of labels.\n",
    "\n",
    "For skills:\n",
    "\n",
    "| method                    | embedded field   |   k |   recall |   precision |   f-score |\n",
    "|:--------------------------|:-----------------|----:|---------:|------------:|----------:|\n",
    "| skill_multiple_embeddings | synthetic query  |  10 |   0.5509 |      0.0861 |    0.1489 |\n",
    "| PREFERREDLABEL        | synthetic query  |  10 |   0.5446 |      0.0877 |    0.1511 |\n",
    "| skill_three_embeddings    | synthetic query  |  10 |   0.5291 |      0.0835 |    0.1443 |\n",
    "| skill_multiple_embeddings | synthetic query  |   5 |   0.4644 |      0.1411 |    0.2164 |\n",
    "| PREFERREDLABEL        | synthetic query  |   5 |   0.4637 |      0.1451 |    0.221  |\n",
    "| skill_three_embeddings    | synthetic query  |   5 |   0.4513 |      0.1388 |    0.2123 |\n",
    "| PREFERREDLABEL        | synthetic query  |   3 |   0.3982 |      0.2018 |    0.2678 |\n",
    "| skill_multiple_embeddings | synthetic query  |   3 |   0.3887 |      0.1894 |    0.2547 |\n",
    "| skill_three_embeddings    | synthetic query  |   3 |   0.3852 |      0.1916 |    0.2559 |\n",
    "| PREFERREDLABEL        | synthetic query  |   1 |   0.2534 |      0.3699 |    0.3007 |\n",
    "| skill_three_embeddings    | synthetic query  |   1 |   0.2329 |      0.3308 |    0.2733 |\n",
    "| skill_multiple_embeddings | synthetic query  |   1 |   0.2248 |      0.3165 |    0.2629 |\n",
    "\n",
    "In case of skills, we notice that lower values of k (1 and 3) correspond to a better performance of single field methods, while the recall of multiple field methods improves for higher values of k. This is possibly a consequence of the fact that skill labels have a more descriptive component than their occupation counterparts, so that when a smaller number of them are chosen, it's easier to confuse the (right) preferred label with a similar (wrong) secondary label or description. On the other hand, having many similar skills to choose from, for higher values of k the choice can be more forgiving and actually pick up secondary labels or descriptions of correct skills that are more similar to the synthetic queries than their preferred label. Notice how in this case, multiple (and not triple) embeddings are particularly advantageous for skills, probably because single secondary labels have less redundancy than in occupations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Can we improve results by adding the Scope Notes field?\n",
    "\n",
    "An additional field that can be found in ESCO databases is the Scope Notes, whose purpose is to explain which use-cases are included or excluded for the particular ESCO node. We want to conduct experiments to verify whether the inclusion of this field change substantially the performance of our retrieval model. In particular, we run the previous experiments on the Hahu dataset for occupations, as the skill Scope Notes field has not been parsed at the time of writing. In particular we are interested in the following questions:\n",
    "1. Can we improve our model performance by including the entirety of the scope notes field? \n",
    "2. Is the performance better if we only consider the \"include\" section?\n",
    "3. In the particular case in which we consider only the \"include\" section, is it better to have a separate embedding for each included case?\n",
    "\n",
    "Before conducting the experiment, it is worth to note that in the Hahu dataset the following is true:\n",
    "- Only 61 out of 542 samples have a nonempty scopenote (11.25% of the dataset)\n",
    "- Only 47 out of 542 samples have an include component of the scopenote (8.67% of the dataset)\n",
    "\n",
    "Therefore, while we expect the improvement to be marginal, we also want to verify if the addition of this extra field has any impact at all.\n",
    "\n",
    "We regenerate the embeddings, since we will now use the Tabiya ESCO dataset version 1.0.0, as this justifies the presence of scope notes because of the added unseen economy components (more in section 4.1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_include_exclude(text):\n",
    "    # Split the text into two parts: before and after \"Excludes:\"\n",
    "    parts = text.split(\"Excludes:\")\n",
    "    \n",
    "    # Extract the includes part by removing the \"Includes:\" header\n",
    "    include = parts[0].replace(\"Includes:\", \"\").strip()\n",
    "    \n",
    "    # Extract the excludes part\n",
    "    exclude = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    \n",
    "    return pd.Series([include, exclude])\n",
    "\n",
    "    \n",
    "def is_not_icatus_leaf(code):\n",
    "    if code.startswith(\"I5\"):\n",
    "        return True\n",
    "    elif code.startswith(\"I\") and not code.endswith(\"0\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Modify path to local accordingly\n",
    "occupation_df_with_scopenotes = pd.read_csv(\"occupations_with_scopenotes.csv\")\n",
    "# Apply the function to the dataframe\n",
    "occupation_df_with_scopenotes[\"CODE\"] = occupation_df_with_scopenotes[\"CODE\"].apply(str)\n",
    "occupation_df_with_scopenotes = occupation_df_with_scopenotes[occupation_df_with_scopenotes[\"CODE\"].apply(is_not_icatus_leaf)]\n",
    "for elem in [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\", \"SCOPENOTE\"]:\n",
    "    occupation_df_with_scopenotes[elem] = occupation_df_with_scopenotes[elem].fillna('')\n",
    "occupation_df_with_scopenotes[['include', 'exclude']] = occupation_df_with_scopenotes['SCOPENOTE'].apply(extract_include_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and create the collection on four embeddings, including the entire SCOPENOTE\n",
    "def create_collection_embeddings(df_database, collection_name, list_unsplit_fields, list_split_fields = [], split_over =[]):\n",
    "    df_full_list = []\n",
    "    for _, row in tqdm(df_database.iterrows()):\n",
    "        for field in list_unsplit_fields:\n",
    "            if row[field]:\n",
    "                df_full_list.append({\"text\": row[field], \"CODE\":row[\"CODE\"]})\n",
    "        for field, split_str in zip(list_split_fields, split_over):\n",
    "            for sec_label in row[field].strip().split(split_str):\n",
    "                if sec_label:\n",
    "                    df_full_list.append({\"text\": sec_label, \"CODE\":row[\"CODE\"]})\n",
    "    full_df = pd.DataFrame(df_full_list)\n",
    "    full_df[\"embeddings\"] = embed_strings_in_batch(list(full_df[\"text\"]), model)\n",
    "    create_collection_in_batch(collection_name, full_df)\n",
    "\n",
    "create_collection_embeddings(occupation_df_with_scopenotes, \"occupation_standard\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\"])\n",
    "create_collection_embeddings(occupation_df_with_scopenotes, \"occupation_scopenote\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\", \"SCOPENOTE\"])\n",
    "create_collection_embeddings(occupation_df_with_scopenotes, \"occupation_include\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\", \"include\"])\n",
    "create_collection_embeddings(occupation_df_with_scopenotes, \"occupation_include_split\", [\"PREFERREDLABEL\", \"DESCRIPTION\", \"ALTLABELS\"], [\"include\"], [\"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full evaluation for titles and synthetic queries. Modify the notebook to save the results locally.\n",
    "occ_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_standard\", \"occupation_scopenote\", \"occupation_include\", \"occupation_include_split\"]:\n",
    "    occ_eval_df = pd.concat([occ_eval_df, run_eval_multiple_embeddings(collection_name, df_occupation_test)])\n",
    "occ_eval_df = occ_eval_df.sort_values(by = [\"k\", \"recall\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows:\n",
    "\n",
    "| method                   | embedded field   |   k |   recall |   precision |   f-score |\n",
    "|:-------------------------|:-----------------|----:|---------:|------------:|----------:|\n",
    "| occupation_standard      | synthetic query  |  10 |   0.7657 |      0.0766 |    0.1392 |\n",
    "| occupation_scopenote     | synthetic query  |  10 |   0.7638 |      0.0764 |    0.1389 |\n",
    "| occupation_include       | synthetic query  |  10 |   0.762  |      0.0762 |    0.1385 |\n",
    "| occupation_include_split | synthetic query  |  10 |   0.7583 |      0.0758 |    0.1379 |\n",
    "| occupation_standard      | synthetic query  |   5 |   0.6863 |      0.1373 |    0.2288 |\n",
    "| occupation_scopenote     | synthetic query  |   5 |   0.6863 |      0.1373 |    0.2288 |\n",
    "| occupation_include       | synthetic query  |   5 |   0.6827 |      0.1365 |    0.2276 |\n",
    "| occupation_include_split | synthetic query  |   5 |   0.679  |      0.1358 |    0.2263 |\n",
    "| occupation_standard      | synthetic query  |   3 |   0.5941 |      0.198  |    0.297  |\n",
    "| occupation_scopenote     | synthetic query  |   3 |   0.5941 |      0.198  |    0.297  |\n",
    "| occupation_include       | synthetic query  |   3 |   0.5886 |      0.1962 |    0.2943 |\n",
    "| occupation_include_split | synthetic query  |   3 |   0.5867 |      0.1956 |    0.2934 |\n",
    "| occupation_standard      | synthetic query  |   1 |   0.369  |      0.369  |    0.369  |\n",
    "| occupation_scopenote     | synthetic query  |   1 |   0.369  |      0.369  |    0.369  |\n",
    "| occupation_include_split | synthetic query  |   1 |   0.3672 |      0.3672 |    0.3672 |\n",
    "| occupation_include       | synthetic query  |   1 |   0.3653 |      0.3653 |    0.3653 |\n",
    "\n",
    "In particular, we can verify that there is no relevant improvement in the addition of the scopenote field for the Hahu test dataset. This was to be expected, given the limited presence of nodes with scope notes in the test set. For this reason, we now focus on the ICATUS dataset on the unseen economy, so that we can get a better idea on the relevancy of scope notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Effectiveness on the ICATUS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ICATUS dataset represents all the nodes of the unseen economy (minus microentrepreneurship) which were added to the Tabiya ESCO version 1.0.0. Having the dataset in its original form, we were also able to formulate queries that would correspond to what users of Brujula would mention if they had performed the occupation corresponding to such node. The original dataset with these queries can then be embedded and used as a test set. Since the Scope Notes for the ICATUS dataset are far more populated, we can now see their impact on vector search.\n",
    "\n",
    "We are now merging the codes to their parent node for all ICATUS element that are not in I5xx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the ICATUS test set (modify the path accordingly)\n",
    "def merge_transform(code):\n",
    "    if code.startswith(\"I5\"):\n",
    "        return code\n",
    "    else:\n",
    "        return f\"{code[:3]}_0\"\n",
    "    \n",
    "icatus_df = pd.read_csv(\"icatus-test-dataset.csv\")\n",
    "icatus_df[\"CODE\"] = icatus_df[\"CODE\"].apply(merge_transform)\n",
    "icatus_df[\"CODE\"] = icatus_df[\"CODE\"].apply(lambda x: [x])\n",
    "icatus_df[\"embeddings\"] = embed_strings_in_batch(list(icatus_df[\"QUERY\"]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation on the full Tabiya ESCO dataset. Modify the script to save the evaluation dataframe locally.\n",
    "occ_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_standard\", \"occupation_scopenote\", \"occupation_include\", \"occupation_include_split\"]:\n",
    "    occ_eval_df = pd.concat([occ_eval_df, run_eval_multiple_embeddings(collection_name, icatus_df)])\n",
    "occ_eval_df = occ_eval_df.sort_values(by = [\"k\", \"recall\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows:\n",
    "\n",
    "| method                   | embedded field   |   k |   recall |   precision |   f-score |\n",
    "|:-------------------------|:-----------------|----:|---------:|------------:|----------:|\n",
    "| occupation_include_split | synthetic query  |  10 |   0.9149 |      0.0915 |    0.1663 |\n",
    "| occupation_include       | synthetic query  |  10 |   0.7872 |      0.0787 |    0.1431 |\n",
    "| occupation_standard      | synthetic query  |  10 |   0.7447 |      0.0745 |    0.1354 |\n",
    "| occupation_scopenote     | synthetic query  |  10 |   0.7447 |      0.0745 |    0.1354 |\n",
    "| occupation_include_split | synthetic query  |   5 |   0.766  |      0.1532 |    0.2553 |\n",
    "| occupation_standard      | synthetic query  |   5 |   0.6596 |      0.1319 |    0.2199 |\n",
    "| occupation_scopenote     | synthetic query  |   5 |   0.6596 |      0.1319 |    0.2199 |\n",
    "| occupation_include       | synthetic query  |   5 |   0.6596 |      0.1319 |    0.2199 |\n",
    "| occupation_include_split | synthetic query  |   3 |   0.6383 |      0.2128 |    0.3191 |\n",
    "| occupation_standard      | synthetic query  |   3 |   0.5319 |      0.1773 |    0.266  |\n",
    "| occupation_scopenote     | synthetic query  |   3 |   0.5319 |      0.1773 |    0.266  |\n",
    "| occupation_include       | synthetic query  |   3 |   0.5319 |      0.1773 |    0.266  |\n",
    "| occupation_include_split | synthetic query  |   1 |   0.3191 |      0.3191 |    0.3191 |\n",
    "| occupation_standard      | synthetic query  |   1 |   0.2553 |      0.2553 |    0.2553 |\n",
    "| occupation_scopenote     | synthetic query  |   1 |   0.2553 |      0.2553 |    0.2553 |\n",
    "| occupation_include       | synthetic query  |   1 |   0.2553 |      0.2553 |    0.2553 |\n",
    "\n",
    "In particular, we observe that the include split is particularly effective for higher values of k (5, 10) and not very relevant for lower values of k, where the standard strategy and scopenote strategy are comparable. This is likely due to the nature of the test set, which has specific queries coinciding with the single elements in the include fields (as they are examples coming from the children nodes). If we expect our application to have a similar structure (so that the declarations would refer to the children nodes, but we need to link them to the parent nodes) we should separate the includes in their field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider the case in which we already know that the query refers to the unseen economy, as this is currently the case in the Brujula algorithm. We can therefore link the Icatus dataset to itself and see how the scopenote impacts the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_icatus(code):\n",
    "    return (code.startswith(\"I\") and code.endswith(\"0\")) or code.startswith(\"I5\")\n",
    "\n",
    "# Filter only elements of the unseen economy (as indicated by their code)\n",
    "occupation_df_unseen = occupation_df_with_scopenotes[occupation_df_with_scopenotes[\"CODE\"].apply(is_icatus)]\n",
    "create_collection_embeddings(occupation_df_unseen, \"occupation_unseen_standard\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\"])\n",
    "create_collection_embeddings(occupation_df_unseen, \"occupation_unseen_scopenote\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\", \"SCOPENOTE\"])\n",
    "create_collection_embeddings(occupation_df_unseen, \"occupation_unseen_include\", [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\", \"include\"])\n",
    "create_collection_embeddings(occupation_df_unseen, \"occupation_unseen_include_split\", [\"PREFERREDLABEL\", \"DESCRIPTION\", \"ALTLABELS\"], [\"include\"], [\"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation on the unseen economy component of the Tabiya ESCO dataset. Modify the script to save the evaluation dataframe locally.\n",
    "occ_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_unseen_standard\", \"occupation_unseen_scopenote\", \"occupation_unseen_include\", \"occupation_unseen_include_split\"]:\n",
    "    occ_eval_df = pd.concat([occ_eval_df, run_eval_multiple_embeddings(collection_name, icatus_df)])\n",
    "occ_eval_df = occ_eval_df.sort_values(by = [\"k\", \"recall\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows:\n",
    "\n",
    "| method                          | embedded field   |   k |   recall |   precision |   f-score |\n",
    "|:--------------------------------|:-----------------|----:|---------:|------------:|----------:|\n",
    "| occupation_unseen_include_split | synthetic query  |  10 |   1      |      0.1    |    0.1818 |\n",
    "| occupation_unseen_scopenote     | synthetic query  |  10 |   0.9574 |      0.0957 |    0.1741 |\n",
    "| occupation_unseen_include       | synthetic query  |  10 |   0.9574 |      0.0957 |    0.1741 |\n",
    "| occupation_unseen_standard      | synthetic query  |  10 |   0.9362 |      0.0936 |    0.1702 |\n",
    "| occupation_unseen_include_split | synthetic query  |   5 |   0.9149 |      0.183  |    0.305  |\n",
    "| occupation_unseen_standard      | synthetic query  |   5 |   0.8936 |      0.1787 |    0.2979 |\n",
    "| occupation_unseen_scopenote     | synthetic query  |   5 |   0.8936 |      0.1787 |    0.2979 |\n",
    "| occupation_unseen_include       | synthetic query  |   5 |   0.8936 |      0.1787 |    0.2979 |\n",
    "| occupation_unseen_standard      | synthetic query  |   3 |   0.8511 |      0.2837 |    0.4255 |\n",
    "| occupation_unseen_scopenote     | synthetic query  |   3 |   0.8511 |      0.2837 |    0.4255 |\n",
    "| occupation_unseen_include_split | synthetic query  |   3 |   0.8511 |      0.2837 |    0.4255 |\n",
    "| occupation_unseen_include       | synthetic query  |   3 |   0.8298 |      0.2766 |    0.4149 |\n",
    "| occupation_unseen_include       | synthetic query  |   1 |   0.7021 |      0.7021 |    0.7021 |\n",
    "| occupation_unseen_standard      | synthetic query  |   1 |   0.6809 |      0.6809 |    0.6809 |\n",
    "| occupation_unseen_scopenote     | synthetic query  |   1 |   0.6596 |      0.6596 |    0.6596 |\n",
    "| occupation_unseen_include_split | synthetic query  |   1 |   0.5532 |      0.5532 |    0.5532 |\n",
    "\n",
    "Notice how these results are pretty similar to those of the previous experiment, with values converging to 1 quickly for larger k, given that we are linking a very limited amount of samples to a very small database. For the same reason as before, the split include field is particularly effective, as it contains explicit examples of the children nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For which value of k do we obtain a sufficiently high recall value?\n",
    "\n",
    "Using the collections defined in the previous section, we now turn to the question of how many occupations should we iterate over if we want to find the correct job. The simplified assumption in this case is that every sample has exactly one correct ESCO node, while this might not be the case in theory. However, by getting a value when the true positive is only one per example gives us an important higher bound on how many occupations are needed to find the correct one in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we would like to aim for a 100% recall. However, since this is not realistic in our experimental setting, we decide to aim for the highest recall such that the search doesn't repeatedly find any additional data as we increase k. The highest recall in this sense is the one that can stay constant for 100 values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_recall(\n",
    "        collection: chromadb.Collection, \n",
    "        df_test: pd.DataFrame,\n",
    "        embedding_column: str,\n",
    "        target_column: str,\n",
    "        n_results: int =100\n",
    "        ) -> int:\n",
    "    \"\"\"Function to find the lowest value of k for which we get\n",
    "    maximized recall (that is, the recall doesn't change for any\n",
    "    larger k up to 100 higher).\n",
    "\n",
    "    Args:\n",
    "        collection (chromadb.Collection): collection from which\n",
    "            we retrieve the occupation.\n",
    "        df_test (pd.DataFrame): test dataframe with codes to retrieve.\n",
    "        embedding_column (str): name of the embedding column in the test\n",
    "            dataframe.\n",
    "        target_column (str): name of the target column in the test dataframe.\n",
    "        n_results (int): number of results to retrieve from the collection.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, float]: lowest value of k giving the highest recall and\n",
    "            highest value of recall\n",
    "    \"\"\"\n",
    "    # Find vector search results\n",
    "    vector_search_results = get_top_n_results_from_embeddings(list(df_test[embedding_column]), collection, n_results)\n",
    "    # Find and order only one result per ESCO code\n",
    "    single_vs_results = [list(set(elem)) for elem in vector_search_results]\n",
    "    vector_search_results_single = [sorted(elem, key=vs_elem.index) for elem, vs_elem in zip(single_vs_results, vector_search_results)]\n",
    "    # Define the lowest value of k and its recall\n",
    "    k=1\n",
    "    rec_at_k=0\n",
    "    # Counter that will be reset everytime the recall improves\n",
    "    improving_counter = 100\n",
    "    while improving_counter>0 and rec_at_k<0.99:\n",
    "        # Calculate recall at k\n",
    "        temp_rec_at_k = recall_at_k(vector_search_results_single, list(df_test[target_column]), k)\n",
    "        # If it's the same as before, reduce the counter\n",
    "        if temp_rec_at_k<=rec_at_k:\n",
    "            improving_counter-=1\n",
    "        # Else reset the counter\n",
    "        else:\n",
    "            improving_counter = 100\n",
    "        # Update recall and k\n",
    "        rec_at_k = temp_rec_at_k\n",
    "        k+=1\n",
    "    if improving_counter==0:\n",
    "        return k-100, rec_at_k\n",
    "    return k, rec_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the k and the recall for the two collections we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k, top_recall = get_highest_recall(client.get_collection(\"occupation_three_embeddings\"), df_occupation_test, \"embeddings\", \"CODE\")\n",
    "print(f\"For the collection with three embeddings per ESCO node, we get a maximum recall of {round(top_recall, 2)} at k={top_k}\")\n",
    "top_k, top_recall = get_highest_recall(client.get_collection(\"occupation_multiple_embeddings\"), df_occupation_test, \"embeddings\", \"CODE\")\n",
    "print(f\"For the collection with more than three embeddings per ESCO node, we get a maximum recall of {round(top_recall, 2)} at k={top_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, there is a trade-off between using more embeddings (thus having more chances of finding the right node) and flooding the number of entries for a single node. In our case, we get higher maximum recalls using only three embeddings, but we get the higher value earlier if we use more than three embeddings. This happens because we retrieve only a limited amount of embeddings at every iteration, so that the ranking is not done over the whole ESCO dataset. We can increase the number of embeddings we retrieve and observe how this trade-off is even more marked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_results in [200, 500, 1000]:\n",
    "    top_k, top_recall = get_highest_recall(client.get_collection(\"occupation_three_embeddings\"), df_occupation_test, \"embeddings\", \"CODE\", n_results)\n",
    "    print(f\"Within the first {n_results} results:\")\n",
    "    print(f\"For the collection with three embeddings per ESCO node, we get a maximum recall of {round(top_recall, 2)} at k={top_k}\")\n",
    "    top_k, top_recall = get_highest_recall(client.get_collection(\"occupation_multiple_embeddings\"), df_occupation_test, \"embeddings\", \"CODE\", n_results)\n",
    "    print(f\"For the collection with more than three embeddings per ESCO node, we get a maximum recall of {round(top_recall, 2)} at k={top_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Evaluating localised ESCO datasets\n",
    "\n",
    "Since we would like to be able to connect users and companies with skills and occupations that are specific to a given country, we now analyze how our method applies to localised data in the example of South Africa. In order to do so, we load a test set containing 1549 SMS in which participants answered to a question about their livelihood and day to day job. These questions are then linked to the localised South Africa ESCO database.\n",
    "\n",
    "The localization process of a given database simply adds secondary labels to existing leaf nodes of the standard ESCO database. No new nodes are added and no other fields than secondary labels are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our evaluation, we want to understand a few things:\n",
    "\n",
    "1. Since localised data is concentrated in the secondary labels, which embedding methods guarantees the highest recall? Should we separate the secondary labels, or keep them together?\n",
    "2. How much is the mapping to the localised ESCO database better than the mapping to the traditional one when it comes to recall in the localised test set?\n",
    "3. When restricting to the subset of the test set which is localised (that is, the ESCO codes whose secondary label is different from the standard), which method performs best?\n",
    "\n",
    "We start by creating two databases in ChromaDB that contain localised data in a similar fashion to the first experiment of this notebook. After calculating the embeddings and regularizing the data, we then evaluate the accuracy on the localised test set for linking to these new databases as well as the old ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a localised dataframe having three rows for each node including the possible fields\n",
    "from tqdm import tqdm \n",
    "\n",
    "df_occupation_sa_list = []\n",
    "for _, row in tqdm(sa_occupation_database_df.iterrows()):\n",
    "    for field in [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\"]:\n",
    "        df_occupation_sa_list.append({\"text\": str(row[field]), \"CODE\":row[\"CODE\"]})\n",
    "df_occupation_sa = pd.DataFrame(df_occupation_sa_list)\n",
    "df_occupation_sa[\"embeddings\"] = embed_strings_in_batch(list(df_occupation_sa[\"text\"]), model)\n",
    "\n",
    "create_collection_in_batch(\"sa_occupation_three_embeddings\", df_occupation_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a localised dataframe having multiple rows for each node including the possible fields\n",
    "# as well as all the separated secondary labels\n",
    "\n",
    "df_occupation_sa_all_list = []\n",
    "for _, row in tqdm(sa_occupation_database_df.iterrows()):\n",
    "    for field in [\"PREFERREDLABEL\", \"DESCRIPTION\"]:\n",
    "        df_occupation_sa_all_list.append({\"text\": row[field], \"CODE\":row[\"CODE\"]})\n",
    "    for sec_label in str(row[\"ALTLABELS\"]).split(\"\\n\"):\n",
    "        df_occupation_sa_all_list.append({\"text\": sec_label, \"CODE\":row[\"CODE\"]})\n",
    "df_occupation_sa_all = pd.DataFrame(df_occupation_sa_all_list)\n",
    "df_occupation_sa_all[\"embeddings\"] = embed_strings_in_batch(list(df_occupation_sa_all[\"text\"]), model)\n",
    "\n",
    "create_collection_in_batch(\"sa_occupation_multiple_embeddings\", df_occupation_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed test data\n",
    "\n",
    "sa_test_df[\"embeddings\"] = embed_strings_in_batch(list(sa_test_df[\"Text\"]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation to standard and localised databases. Modify the notebook to save the results locally.\n",
    "\n",
    "eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_multiple_embeddings\", \"occupation_three_embeddings\", \"sa_occupation_multiple_embeddings\", \"sa_occupation_three_embeddings\"]:\n",
    "    eval_df = pd.concat([eval_df, run_eval_multiple_embeddings(collection_name, sa_test_df)])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the evaluation are as follows:\n",
    "\n",
    "| method                              | k   | recall | precision | f-score |\n",
    "|-------------------------------------|-----|--------|-----------|---------|\n",
    "| occupation_three_embeddings         | 10  | 0.4551 | 0.0455    | 0.0828  |\n",
    "| sa_occupation_three_embeddings      | 10  | 0.4532 | 0.0453    | 0.0824  |\n",
    "| sa_occupation_multiple_embeddings   | 10  | 0.4513 | 0.0451    | 0.082   |\n",
    "| occupation_multiple_embeddings      | 10  | 0.4041 | 0.0404    | 0.0735  |\n",
    "| occupation_three_embeddings         | 5   | 0.366  | 0.0732    | 0.122   |\n",
    "| sa_occupation_three_embeddings      | 5   | 0.3635 | 0.0727    | 0.1212  |\n",
    "| sa_occupation_multiple_embeddings   | 5   | 0.3402 | 0.068     | 0.1134  |\n",
    "| occupation_multiple_embeddings      | 5   | 0.3338 | 0.0668    | 0.1113  |\n",
    "| sa_occupation_three_embeddings      | 3   | 0.3015 | 0.1005    | 0.1507  |\n",
    "| occupation_three_embeddings         | 3   | 0.2989 | 0.0996    | 0.1495  |\n",
    "| sa_occupation_multiple_embeddings   | 3   | 0.2699 | 0.09      | 0.1349  |\n",
    "| occupation_multiple_embeddings      | 3   | 0.2666 | 0.0889    | 0.1333  |\n",
    "| occupation_three_embeddings         | 1   | 0.1827 | 0.1827    | 0.1827  |\n",
    "| sa_occupation_three_embeddings      | 1   | 0.1821 | 0.1821    | 0.1821  |\n",
    "| occupation_multiple_embeddings      | 1   | 0.1646 | 0.1646    | 0.1646  |\n",
    "| sa_occupation_multiple_embeddings   | 1   | 0.1472 | 0.1472    | 0.1472  |\n",
    "\n",
    "As we can see, the method with three embeddings outperforms the one in which the labels are distinct, both when using the localised version of the database and when using the non-localised. As in the previous example for the standard version, this advantage decreases the larger the k, which is to be expected.\n",
    "\n",
    "Moreover, at the current state, the non-localised version of ESCO is working quite well in identifying the correct node. This might happen for a few reasons, that could be further investigated:\n",
    "1. The test set links mostly to nodes that are not localised.\n",
    "2. The test set links to nodes that are localised, but the text doesn't refer to the localised secondary labels.\n",
    "3. The methods considered (embeddings or segmentation) don't perform well on localised data. \n",
    "\n",
    "As we observe in the next iteration, most of the test set links to localised nodes, so the reason is definitely not the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the subset of the test set that links to localised nodes\n",
    "localised_node_list = []\n",
    "for _, row in sa_occupation_database_df.iterrows():\n",
    "    original_node = df_occupation_database[df_occupation_database[\"CODE\"]==row[\"CODE\"]]\n",
    "    if len(original_node) == 1:\n",
    "        original_node = original_node.iloc[0]\n",
    "        if original_node[\"DESCRIPTION\"]!=row[\"DESCRIPTION\"] or original_node[\"PREFERREDLABEL\"]!=row[\"PREFERREDLABEL\"] or original_node[\"ALTLABELS\"]!=row[\"ALTLABELS\"]:\n",
    "            localised_node_list.append(row[\"CODE\"])\n",
    "    elif len(original_node) > 1:\n",
    "        raise Exception   \n",
    "\n",
    "localised_sa_test_df = sa_test_df[sa_test_df[\"Esco Code\"].apply(str).isin(localised_node_list)]\n",
    "print(f\" We have a total of {len(localised_sa_test_df)} localised samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate linking to the localised dataframe. Modify the notebook to save the results locally.\n",
    "\n",
    "localised_eval_df = pd.DataFrame()\n",
    "for collection_name in [\"occupation_multiple_embeddings\", \"occupation_three_embeddings\",\"sa_occupation_multiple_embeddings\", \"sa_occupation_three_embeddings\"]:\n",
    "    localised_eval_df = pd.concat([localised_eval_df, run_eval_multiple_embeddings(collection_name, localised_sa_test_df)])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the evaluation are as follows:\n",
    "\n",
    "| method                              | k   | recall | precision | f-score |\n",
    "|-------------------------------------|-----|--------|-----------|---------|\n",
    "| occupation_three_embeddings         | 10  | 0.4393 | 0.0439    | 0.0799  |\n",
    "| sa_occupation_multiple_embeddings   | 10  | 0.4393 | 0.0439    | 0.0799  |\n",
    "| sa_occupation_three_embeddings      | 10  | 0.4372 | 0.0437    | 0.0795  |\n",
    "| occupation_multiple_embeddings      | 10  | 0.3883 | 0.0388    | 0.0706  |\n",
    "| occupation_three_embeddings         | 5   | 0.3476 | 0.0695    | 0.1159  |\n",
    "| sa_occupation_three_embeddings      | 5   | 0.3448 | 0.069     | 0.1149  |\n",
    "| sa_occupation_multiple_embeddings   | 5   | 0.3283 | 0.0657    | 0.1094  |\n",
    "| occupation_multiple_embeddings      | 5   | 0.32   | 0.064     | 0.1067  |\n",
    "| sa_occupation_three_embeddings      | 3   | 0.2814 | 0.0938    | 0.1407  |\n",
    "| occupation_three_embeddings         | 3   | 0.2786 | 0.0929    | 0.1393  |\n",
    "| sa_occupation_multiple_embeddings   | 3   | 0.2614 | 0.0871    | 0.1307  |\n",
    "| occupation_multiple_embeddings      | 3   | 0.2538 | 0.0846    | 0.1269  |\n",
    "| occupation_three_embeddings         | 1   | 0.169  | 0.169     | 0.169   |\n",
    "| sa_occupation_three_embeddings      | 1   | 0.1683 | 0.1683    | 0.1683  |\n",
    "| occupation_multiple_embeddings      | 1   | 0.1559 | 0.1559    | 0.1559  |\n",
    "| sa_occupation_multiple_embeddings   | 1   | 0.1386 | 0.1386    | 0.1386  |\n",
    "\n",
    "Since the size of the dataset is not changed much from the previous iteration, we can find the same conclusions. Further work will need to be conducted to establish whether the test set refers explicitly to the secondary labels and in case whether there are more effective methods to highlight the localised secondary labels during the search. Additionally, we should discuss with researchers the semantic relationship between the new secondary labels and the requests, highlighting which samples in the test sets refer explicitly to such labels. Finally, we should wait for an updated version of the localised South African ESCO database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Linking to French ESCO\n",
    "\n",
    "We run an evaluation to study the best strategy to link french sentences to the ESCO occupation database. Since we don't have a french test set, we generated synthetic queries in French by translating the synthetic queries from English and considered two different strategies:\n",
    "\n",
    "1. Use the multilingual embedding model to embed the french occupation ESCO model and the french synthetic. The nodes and the queries would then be compared using semantic similarity so that the most similar nodes could be retrieved.\n",
    "2. Use Gemini to translate the query from French to English, embed it using the regular Gecko model and then link it to the English ESCO model via semantic similarity. The translation from French to English were pre-computed to speed up the process and avoid excessive costs.\n",
    "\n",
    "It's important to notice that because we don't have queries that are originally in French, the double translation could generate some bias as the original sentence came from English. We ignore the bias at the moment, but will keep it in mind for future applications.\n",
    "\n",
    "Since we are using the same test set of Hahu jobs occupations, we will also compare the outcome with the original English version. Note that we have chosen the best performing method for English (embedding the three fields separately) for the French case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a localised dataframe having three rows for each node including the possible fields\n",
    "\n",
    "multi_model = TextEmbeddingModel.from_pretrained(\"text-multilingual-embedding-002\")\n",
    "df_occupation_fr_list = []\n",
    "for _, row in tqdm(df_occupation_database_fr.iterrows()):\n",
    "    for field in [\"PREFERREDLABEL\", \"ALTLABELS\", \"DESCRIPTION\"]:\n",
    "        df_occupation_fr_list.append({\"text\": str(row[field]), \"CODE\":row[\"CODE\"]})\n",
    "df_occupation_fr = pd.DataFrame(df_occupation_fr_list)\n",
    "df_occupation_fr[\"embeddings\"] = embed_strings_in_batch(list(df_occupation_fr[\"text\"]), multi_model)\n",
    "\n",
    "create_collection_in_batch(\"fr_occupation_three_embeddings\", df_occupation_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_test_df[\"embeddings\"] = embed_strings_in_batch(list(fr_test_df[\"fr_synthetic_query\"]), multi_model)\n",
    "fr_test_df[\"fr_to_en_embeddings\"] = embed_strings_in_batch(list(fr_test_df[\"fr_to_en_synthetic_query\"]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the evaluation. Modify the notebook to save the results locally\n",
    "\n",
    "eval_df1 = run_eval_multiple_embeddings(\"fr_occupation_three_embeddings\", fr_test_df)\n",
    "eval_df2 = run_eval_multiple_embeddings(\"occupation_three_embeddings\", fr_test_df,embedding_column=\"fr_to_en_embeddings\")\n",
    "eval_df = pd.concat([eval_df1, eval_df2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested two possible strategies for embedding French sentences:\n",
    "1. Embed French ESCO using the multilingual ESCO model (Link to French)\n",
    "2. Translate a French sentence to English and link to the English ESCO (Translate to English)\n",
    "\n",
    "We compared these two strategies with the original best performing strategy and realised that the translation loses some information from the original, but that translating a french sentence to English and then linking performs better than directly linking to the french version of the database. This difference is particularly strong for low levels of k and decreases as k goes up.\n",
    "\n",
    "| Method                  | Embedded Field   | k  | Recall | Precision | F-Score |\n",
    "|-------------------------|------------------|----|--------|-----------|---------|\n",
    "| Original Eng to Eng     |  synthetic query | 10 |0.7601  | 0.076     | 0.1382  |\n",
    "| Translate to English    | fr_to_en_synthetic query  | 10 | 0.7196 | 0.072 | 0.1308 |\n",
    "| Link to French          | fr_synthetic query  | 10 | 0.6919 | 0.0692 | 0.1258 |\n",
    "| Original Eng to Eng     |  synthetic query | 5 |0.6919  | 0.1384     | 0.2306  |\n",
    "| Translate to English    | fr_to_en_synthetic query  | 5  | 0.6328 | 0.1266 | 0.2109 |\n",
    "| Link to French          | fr_synthetic query  | 5  | 0.5756 | 0.1151 | 0.1919 |\n",
    "| Original Eng to Eng     |  synthetic query | 3 |0.5959  | 0.1986     | 0.298  |\n",
    "| Translate to English    | fr_to_en_synthetic query  | 3  | 0.559  | 0.1863 | 0.2795 |\n",
    "| Link to French          | fr_synthetic query  | 3  | 0.5037 | 0.1679 | 0.2518 |\n",
    "| Original Eng to Eng     | synthetic query  | 1  | 0.3819 | 0.3819    | 0.3819  | \n",
    "| Translate to English    | fr_to_en_synthetic query  | 1  | 0.3635 | 0.3635 | 0.3635 |\n",
    "| Link to French          | fr_synthetic query  | 1  | 0.3063 | 0.3063 | 0.3063 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The French model is actually localized in a similar way to the South African model, as secondary labels are added depending on the localized nature of the profession (i.e. ESCO occupation 7512.5 for pastry chef or ptissier/ptissire).\n",
    "\n",
    "The loss of recall could therefore be caused by the difference in secondary labels that is actually mismatching the linking process. In order to control for this difference and evaluate properly the multilingual embedding model, we embed only the principal label and the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupation_fr_list = []\n",
    "for _, row in tqdm(df_occupation_database_fr.iterrows()):\n",
    "    for field in [\"PREFERREDLABEL\", \"DESCRIPTION\"]:\n",
    "        df_occupation_fr_list.append({\"text\": str(row[field]), \"CODE\":row[\"CODE\"]})\n",
    "df_occupation_fr = pd.DataFrame(df_occupation_fr_list)\n",
    "df_occupation_fr[\"embeddings\"] = embed_strings_in_batch(list(df_occupation_fr[\"text\"]), multi_model)\n",
    "\n",
    "create_collection_in_batch(\"fr_occupation_two_embeddings\", df_occupation_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupation_list = []\n",
    "for _, row in tqdm(df_occupation_database.iterrows()):\n",
    "    for field in [\"PREFERREDLABEL\", \"DESCRIPTION\"]:\n",
    "        df_occupation_list.append({\"text\": str(row[field]), \"CODE\":row[\"CODE\"]})\n",
    "df_occupation = pd.DataFrame(df_occupation_list)\n",
    "df_occupation[\"embeddings\"] = embed_strings_in_batch(list(df_occupation[\"text\"]), model)\n",
    "\n",
    "create_collection_in_batch(\"occupation_two_embeddings\", df_occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df1 = run_eval_multiple_embeddings(\"fr_occupation_two_embeddings\", fr_test_df)\n",
    "eval_df2 = run_eval_multiple_embeddings(\"occupation_two_embeddings\", df_occupation_test)\n",
    "\n",
    "eval_df = pd.concat([eval_df1, eval_df2])\n",
    "eval_df.to_csv(\"/Users/francescopreta/coding/compass/backend/esco_search/_scripts/evalfrench.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results, compared to the English linking results are as follows:\n",
    "\n",
    "| method                       | k  | recall | precision | f-score |\n",
    "|------------------------------|----|--------|-----------|---------|\n",
    "| occupation_two_embeddings    | 10 | 0.7565 | 0.0756    | 0.1375  |\n",
    "| fr_occupation_two_embeddings | 10 | 0.6771 | 0.0677    | 0.1231  |\n",
    "| occupation_two_embeddings    | 5  | 0.6716 | 0.1343    | 0.2239  |\n",
    "| fr_occupation_two_embeddings | 5  | 0.559  | 0.1118    | 0.1863  |\n",
    "| occupation_two_embeddings    | 3  | 0.5849 | 0.195     | 0.2924  |\n",
    "| fr_occupation_two_embeddings | 3  | 0.4797 | 0.1599    | 0.2399  |\n",
    "| occupation_two_embeddings    | 1  | 0.369  | 0.369     | 0.369   |\n",
    "| fr_occupation_two_embeddings | 1  | 0.3063 | 0.3063    | 0.3063  |\n",
    "\n",
    "Clearly the use of the multilanguage model on the analogous fields determines a similar recall loss to the one obtained with the embedding of all three fields. In other words, by controlling for secondary labels, we find out that the difference between the English and the multilingual embedding models is the same as before, although the absolute values are shifted by a couple of percentage points at every value of k. This shows that the multilingual model determines in fact a strong performance drop. Moreover, the addition of secondary labels to the French model guarantees a performance increase that is very similar to the one for the English model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The purpose of our study has been to evaluate linking models to the Occupation and Skill ESCO taxonomies. We chose to do so by generating synthetic queries based on a dataset of 542 job descriptions with the corresponding ESCO code for the occupations and of 1054 sentences containing 2013 skills with the corresponding Tabiya UUID. Those synthetic queries are assumed to be similar to the input that an hypothetical user of the Brujula platform would submit. We also fixed the embedding model to be VertexAI's Gecko003.\n",
    "\n",
    "We first focused the evaluation on a selection of hyperparameters that would guarantee the maximum recall at various values of retrieved nodes k. We assumed that k would be decided in advance and that we would prioritize retrieving the ground truth within the first k documents (recall-based approach) rather than having most retrieved documents be the ground truth (precision-based approach). This would be the case because we could add a validation step in which the user would confirm which skills would be relevant to their experience. The hyperparameter we considered were \n",
    "* The score function (euclidean distance, scalar product or cosine similarity);\n",
    "* The usage of Maximal Marginal Relevance (MMR) or not to choose the top documents;\n",
    "* How to embed each node as a combination of the collection fields.\n",
    "\n",
    "We found that both for the occupations and for the skills, it made no difference which score function should be used. Moreover, we found that most correct nodes could be found within the first few documents, so that using MMR would not be beneficial to our purpose. Finally we discovered that using only the preferred label guarantees a higher recall at all values of k, with the performance matched by using a combination of all fields for higher values of k. This happens most likely because the label encodes the core meaning of the job, even when it's not referred to directly. When given the possibility to make mistakes, the best match is sometimes more similar to the secondary labels (which are sometimes not unique) or to the description, which explains the result for higher k.\n",
    "\n",
    "The second evaluation involved understanding how to retrieve skills that are connected to a given occupation. A first naive approach involved linking the occupation synthetic queries to the skill database and calculating the recall using the essential skills related to the ground truth occupation as true values. This however, was not very efficient for our values of k as each occupation has an average of 27 related essential skills. Of the top 10 skills, less than 2 on average were included in these essential skills, so we decided to change approach.\n",
    "\n",
    "The approach that yielded better results involved linking occupations first and then considering as predictions all the essential skills related to the occupations, compared to the ground truth of all the essential skills of the true occupation. It is clear that the recall of this method would be higher than the recall of the occupation linking evaluation, as a correctly retrieved occupation would guarantee that all its essential skills are correctly retrieved. However, the interesting component is seeing how wrongly retrieved occupations might have skills in common with the correct one, thus improving the overall recall of skills. This is indeed already the case for k=1, so that about half the essential skills are retrieved on average. For higher values of k, this leads to very large recall values at the expenses of the precision. We included the F-score to make sure we could choose the k that would maximizes this trade-off, which is a value between 1 and 3. The method could be further improved by adding a ranking system tha would present the most relevant skills for measures given outside of this experiment (such as relevance, transferability and so on).\n",
    "\n",
    "Moreover, we were interested in understanding whether we would benefit from a Named Entity Recognition (NER) model that would select subspans of the query to be linked to the occupation. We did so by linking only the titles and comparing their results to the ground truth, both for the occupation evaluation and for the essential skills of the second experiment. We found that indeed linking the title guarantees an increment in recall for low values of k, but that this gains tend to disappear for higher values of k. This suggests that we might benefit from a NER model if we decide to retrieve a lower number of elements. We also analyzed the residual elements in which the title linking failed and found out that linking the whole sentence to the description was more successful than linking it to the preferred label. We suggested for this reason that if a NER model were to be implemented, we could consider linking the retrieved subspan to the preferred label and the sentences in which no span is retrieved to a combination of all the fields.\n",
    "\n",
    "Another line of work consisted in studying whether strategies including multiple embeddings per node would yield better results than strategies including only one embedding. We restricted to the occupation evaluation and compared our previous results to a strategy embedding separately the three fields for preferred label, secondary labels and description and to another strategy embedding the preferred label, and description fields as well as all separate secondary label for each node. We found that the one with three embeddings outperforms both the highest performing single embedding strategy, as well as the one with multiple embeddings, both when linking the synthetic query and the title.\n",
    "\n",
    "The fifth experiment consisted in understanding how many documents we need to retrieve to guarantee to have the true positive for all of our samples in the test set. We run this experiment using the best retrieval function on occupation (three embeddings per node) and run into some technical issues that could not guarantee a 100% recall. However, we found that our upper bound was around k=82. This seems to imply that it is not feasible to analyze the top 82 elements with brute force, but we should probably find a strategy to rank our results after multiple questions to retrieve the correct one. Moreover, it might not be appropriate to assume that there is only one correct ESCO node for each user query, therefore we should investigate how to restrict to those that are relevant, which could be found in a much smaller number of tries.\n",
    "\n",
    "Additionally, we focused on the evaluation of our embedding method on localised ESCO data for South Africa, seeing if the method could generalize to other dataset. We used a test set provided by the University of Oxford and consisting of 1549 SMS in which users replied to a question about their livelihood. Those replies were later identified with an ESCO code matching the data in a localised ESCO database for South Africa. We replicated our evaluation trying to observe if we could link the test set to the database, although we observed that the linking performance to the original ESCO database is very similar to the one for the localised ESCO. This could happen for a few reasons: either the data doesn't contain many samples linking to localised nodes, or the construction of the database doesn't reflect new knowledge included in the sample or finally because our linking methods are not sophisticated enough to pick up the differences. We verified that indeed 1450 out of the 1549 SMS are linking to localised ESCO nodes, so that the reason might be one of the other two. Since the development of a definitive South African localised ESCO is still underway, we refrain from conducting further evaluation.\n",
    "\n",
    "Finally, we considered the evaluation of ESCO in the French language as a way to compare the regular gecko embedding model with its multilingual version. We found that the multilingual model determined a loss of recall of about 8% from the regular model, both in the general case of embedding all three fields of ESCO nodes, and in the case in which we removed secondary labels as they could be different for localised nodes. We found the difference to be the same in both cases, leading to determine that the multilingual embedding model has a worse predictive power than the English one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
