{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills Data Preparation\n",
    "\n",
    "We focus on the evaluation of an embedding model as a method to retrieve the correct ESCO code for skill requirements listed in job postings. The `skillspan` test dataset consists of 2406 skills contained in 920 sentences extracted from job postings, while the `techwolf` dataset contains 588 skills extracted from 326 sentence. Each row of the dataset contains:\n",
    "- the `sentence` of interest;\n",
    "- the continuous `span` of the sentence referring to the skill of interest\n",
    "- the (non necessarily continuous) `subspan` of the span referring to the specific skill. This column has None value when it coincides with the span.\n",
    "- the ESCO `label` of the leaf corresponding to the skill of interest.\n",
    "\n",
    "The `techwolf` dataset has None values for all of the `span` and `subspan`. This does not affect our outcome, since we will not be using these columns.\n",
    "\n",
    "Since leaf nodes of ESCO don't have an ESCO code, we will need to identify them by associating a unique UUID corresponding to the one found in the Tabiya ESCO 1.1.1 version. We will also associate a unique ID to each row so that artifacts can be made out of them. Finally, we will create a synthetic query that resembles our practical application in the Compass interface for a better evaluation. In practice we will:\n",
    "\n",
    "1. Load the test datasets;\n",
    "2. For each dataset, assign a unique `ID` to each row;\n",
    "3. Load the Tabiya ESCO skills version 1.1.1;\n",
    "4. For each row of each dataset, assign the `UUID` corresponding to the label;\n",
    "5. Verify that each `label` corresponds to a unique `UUID` and remove the rows where the `label` doesn't match a `UUID`;\n",
    "6. Merge the two datasets.\n",
    "7. Generate a synthetic query using Google Vertex AI.\n",
    "8. Save the resulting artifacts into the Tabiya Huggingface repository. These will consist of the original artefacts with IDs, as well as a unique merged artefact for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescopreta/miniconda3/envs/backend/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading the test datasets for skills using the Huggingface library\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"/Users/francescopreta/coding/compass/backend/.env\")\n",
    "\n",
    "HF_TOKEN = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "REPO_ID = \"tabiya/esco_skills_test\"\n",
    "FILENAME_SK = \"data/skillspan-00000-of-00001.parquet\"\n",
    "FILENAME_TE = \"data/techwolf-00000-of-00001.parquet\"\n",
    "\n",
    "test_df_sk = pd.read_parquet(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME_SK, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n",
    "test_df_te = pd.read_parquet(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME_TE, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Assign a unique ID to each row.\n",
    "test_df_sk['ID'] = test_df_sk.reset_index().index\n",
    "test_df_sk['ID'] = test_df_sk['ID'].apply(lambda x: f\"sk-{x}\")\n",
    "test_df_te['ID'] = test_df_te.reset_index().index\n",
    "test_df_te['ID'] = test_df_te['ID'].apply(lambda x: f\"te-{x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the Tabiya ESCO skills version 1.1.1 from github\n",
    "SKILL_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/main/data-sets/csv/tabiya-esco-v1.1.1/skills.csv\"\n",
    "\n",
    "esco_df = pd.read_csv(SKILL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For each row, assign the UUID corresponding to the PREFERREDLABEL\n",
    "# We will also verify that there are no duplicates of PREFERREDLABEL\n",
    "\n",
    "label_to_uuid = {}\n",
    "for _, row in esco_df.iterrows():\n",
    "    label = row[\"PREFERREDLABEL\"]\n",
    "    if label not in label_to_uuid:\n",
    "        label_to_uuid[label] = row[\"UUIDHISTORY\"]\n",
    "    else:\n",
    "        raise ProcessLookupError(\"Preferred label has more than one UUID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. We generate a new column in the test set with the UUID corresponding to the label\n",
    "# We remove each row of the test set that doesn't have a corresponding UUID\n",
    "test_df_sk[\"UUID\"] = test_df_sk[\"label\"].apply(lambda x: None if x not in label_to_uuid else label_to_uuid[x])\n",
    "test_df_te[\"UUID\"] = test_df_te[\"label\"].apply(lambda x: None if x not in label_to_uuid else label_to_uuid[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981\n",
      "{'UNK'}\n"
     ]
    }
   ],
   "source": [
    "# The original skillspan dataset contains 981 rows in which the skill label is unknown (UNK).\n",
    "# We create a second file removing the lines which will be not needed for our evaluation\n",
    "print(len(test_df_sk[test_df_sk[\"UUID\"].isnull()][\"label\"]))\n",
    "print(set(test_df_sk[test_df_sk[\"UUID\"].isnull()][\"label\"]))\n",
    "\n",
    "test_df_sk_updated = test_df_sk.dropna(subset=[\"UUID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. We merge the two datasets\n",
    "\n",
    "test_df = pd.concat([test_df_sk_updated, test_df_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 216/1054 [03:56<15:15,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142    - You must be highly driven with aspirations o...\n",
      "Name: - You must be highly driven with aspirations of becoming a partner., dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 728/1054 [13:36<05:43,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418    The tasks can be feeding moving castration ins...\n",
      "1421    The tasks can be feeding moving castration ins...\n",
      "1422    The tasks can be feeding moving castration ins...\n",
      "Name: The tasks can be feeding moving castration insemination monitoring the animals health and others ., dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1054/1054 [19:46<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# 7. We generate a synthetic query on the merged test dataframe obtained as an answer\n",
    "# to the question 'What are your skills and expertise? Answer in one sentence\n",
    "\n",
    "from time import sleep\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_text(sentence: str) -> str:\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init()\n",
    "    # Load the model\n",
    "    generative_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "    # Query the model\n",
    "    response = generative_model.generate_content(\n",
    "        [\n",
    "            f\"Given the following \\\n",
    "            description of the user's skill, return the answer of \\\n",
    "            the user to the following question.\\n\\n\\\n",
    "            Description:\\n{sentence}\\n\\n\\\n",
    "            Question: 'What are your skills and expertise? Answer in one sentence. Don't be too formal.\\n\\n\\\n",
    "            Answer: \",\n",
    "        ]\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "test_df[\"synthetic_query\"] = test_df.groupby(\"sentence\")[\"sentence\"].progress_transform(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skillspan_with_id.parquet: 100%|██████████| 149k/149k [00:00<00:00, 162kB/s]\n",
      "techwolf_with_id.parquet: 100%|██████████| 47.6k/47.6k [00:00<00:00, 78.4kB/s]\n",
      "processed_skill_test_set_with_id.parquet: 100%|██████████| 217k/217k [00:00<00:00, 228kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tabiya/esco_skills_test/commit/3b8516b18599aa23dda63cc6ef287826b7228ecd', commit_message='Upload data/processed_skill_test_set_with_id.parquet with huggingface_hub', commit_description='', oid='3b8516b18599aa23dda63cc6ef287826b7228ecd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. We upload the original files with ID and the updated merged file with synthetic queries\n",
    "# on the huggingface repository\n",
    "from huggingface_hub import HfApi\n",
    "import tempfile\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "with tempfile.TemporaryFile() as temp:\n",
    "    test_df_sk.to_parquet(temp.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp.name,\n",
    "        path_in_repo=\"data/skillspan_with_id.parquet\",\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "with tempfile.TemporaryFile() as temp2:\n",
    "    test_df_te.to_parquet(temp2.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp2.name,\n",
    "        path_in_repo=\"data/techwolf_with_id.parquet\",\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "with tempfile.TemporaryFile() as temp3:\n",
    "    test_df.to_parquet(temp3.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp3.name,\n",
    "        path_in_repo=\"data/processed_skill_test_set_with_id.parquet\",\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
