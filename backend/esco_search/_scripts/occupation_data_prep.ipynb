{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations Data Preparation\n",
    "\n",
    "We focus on the evaluation of an embedding model as a method to retrieve the correct ESCO occupation label for a given job posting. The test dataset consists of 550 job postings with titles, brief description, general classification and ESCO code. \n",
    "\n",
    "In our use-case we will not need to classify from a description, but rather from a user query in a conversation with a LLM, we will need to use this data to generate a synthetic user response that will then be used for evaluation. However, the description contains personal and private information, so we will use the google Cloud Data Loss Prevention (DLP) API to mask those information. \n",
    "\n",
    "We want to be able to test a linker for the occupation, as well as one for the skills. For that reason, we will also save with each ESCO code the related skills UUID as found in the Tabiya ESCO v1.1.1 database. In practice we will:\n",
    "\n",
    "1. Load the test dataset;\n",
    "2. Assign a unique ID to each row;\n",
    "3. Pass all the descriptions to the Google DLP API to mask private information;\n",
    "4. Pass the titles and descriptions to a LLM to generatic synthetic queries that could resemble how the user interacts with our platform;\n",
    "5. Load skills, occupations and occupation to skill dataset;\n",
    "6. Save the essential and optional related skills to each occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading the test dataset for occupations using the Huggingface library\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "REPO_ID = \"tabiya/hahu_test\"\n",
    "FILENAME = \"hahu_test.csv\"\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Assign a unique ID to each row.\n",
    "test_df['ID'] = test_df.reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [04:22<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. Defining the function to remove personal and private information from the description field.\n",
    "from google.cloud import dlp_v2\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def deidentify_text(text):\n",
    "    # Initialize a DLP client\n",
    "    dlp_client = dlp_v2.DlpServiceClient()\n",
    "\n",
    "    # Construct the info types to redact\n",
    "    info_types = [\n",
    "        {'name': 'EMAIL_ADDRESS'},\n",
    "        {'name': 'PHONE_NUMBER'},\n",
    "        {'name': 'PERSON_NAME'},\n",
    "        {'name': 'LOCATION'},\n",
    "        {'name': 'ORGANIZATION_NAME'},\n",
    "        {'name': 'STREET_ADDRESS'}\n",
    "    ]\n",
    "\n",
    "    # Construct the deidentification configuration\n",
    "    inspect_config = {'info_types': info_types, \"min_likelihood\": dlp_v2.Likelihood.POSSIBLE,}\n",
    "    deidentify_config = {\n",
    "        'info_type_transformations': {\n",
    "            'transformations': [\n",
    "                {\n",
    "                    'primitive_transformation': {\n",
    "                        'replace_config': {\n",
    "                            'new_value': {\n",
    "                                'string_value': '[REDACTED]'\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert text input to DLP API request format\n",
    "    item = {\"value\": text}\n",
    "\n",
    "    # Construct the request\n",
    "    request = {\n",
    "        \"parent\": f\"projects/{os.environ['GOOGLE_PROJECT_ID']}/locations/global\",\n",
    "        \"deidentify_config\": deidentify_config,\n",
    "        \"inspect_config\": inspect_config,\n",
    "        \"item\": item\n",
    "    }\n",
    "\n",
    "    # Call the API\n",
    "    response = dlp_client.deidentify_content(request=request)\n",
    "\n",
    "    # Return the deidentified text\n",
    "    return response.item.value\n",
    "\n",
    "\n",
    "# Determine the redacted description\n",
    "test_df[\"redacted_description\"] = test_df[\"description\"].progress_apply(deidentify_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generating synthetic query for linking\n",
    "from time import sleep\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "def generate_text(title: str, description: str) -> str:\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init()\n",
    "    # Load the model\n",
    "    generative_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "    # Query the model\n",
    "    response = generative_model.generate_content(\n",
    "        [\n",
    "            f\"Given the following \\\n",
    "            description of the user's past job, return the answer of \\\n",
    "            the user to the following question.\\n\\n\\\n",
    "            Description:\\n{title}\\n{description}\\n\\n\\\n",
    "            Question: Describe your last job. Answer in one sentence. Don't be too formal.\\n\\n\\\n",
    "            Answer: \",\n",
    "        ]\n",
    "    )\n",
    "    # Introduce a delay to make sure we don't send \n",
    "    # too many requests\n",
    "    sleep(5)\n",
    "    return response.text\n",
    "\n",
    "test_df[\"synthetic_query\"] = test_df.progress_apply(lambda x: generate_text(x[\"title\"], x[\"redacted_description\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load the skills, occupations and occupation to skills dataset from github\n",
    "SKILL_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/main/data-sets/csv/tabiya-esco-v1.1.1/skills.csv\"\n",
    "OCCUPATION_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/taxonomy-model-application/main/data-sets/csv/tabiya-esco-v1.1.1/occupations.csv\"\n",
    "OCCUPATION_TO_SKILL_DATA_PATH = \"https://raw.githubusercontent.com/tabiya-tech/tabiya-open-dataset/main/tabiya-esco-v1.1.1/csv/occupation_skill_relations.csv\"\n",
    "\n",
    "df_skills = pd.read_csv(SKILL_DATA_PATH)\n",
    "df_occupation = pd.read_csv(OCCUPATION_DATA_PATH)\n",
    "df_occupation_to_skills = pd.read_csv(OCCUPATION_TO_SKILL_DATA_PATH)\n",
    "\n",
    "# We save the occupation to skills map, distinguishing between essential and optional\n",
    "esco_code_to_occupation_id = {row[\"CODE\"]:row[\"ID\"] for _, row in df_occupation.iterrows()}\n",
    "skill_id_to_uuid = {row[\"ID\"]: row[\"UUIDHISTORY\"] for _, row in df_skills.iterrows()}\n",
    "grouped_df = df_occupation_to_skills.groupby([\"OCCUPATIONID\",\"RELATIONTYPE\"])[\"SKILLID\"].agg(list).reset_index()\n",
    "occupation_id_to_skills_essential = {row[\"OCCUPATIONID\"]:row[\"SKILLID\"] for _, row in grouped_df.iterrows() if row[\"RELATIONTYPE\"]==\"essential\"}\n",
    "occupation_id_to_skills_optional = {row[\"OCCUPATIONID\"]:row[\"SKILLID\"] for _, row in grouped_df.iterrows() if row[\"RELATIONTYPE\"]==\"optional\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing occupation ESCO codes:  ['1120.2', '1120.2', '1120.2', '2263.4', '3212.1', '2141.7', '2141.7', '1120.2.1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/zdgbwzss4dd84x7mp9nz9nj00000gn/T/ipykernel_11189/2930152486.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  redacted_test_df[\"skills_essential\"] = skills_essential\n",
      "/var/folders/hy/zdgbwzss4dd84x7mp9nz9nj00000gn/T/ipykernel_11189/2930152486.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  redacted_test_df[\"skills_optional\"] = skills_optional\n"
     ]
    }
   ],
   "source": [
    "# 6. Save the essential and optional related skills to each occupation.\n",
    "\n",
    "missing_occupation_codes = []\n",
    "skills_essential = []\n",
    "skills_optional = []\n",
    "for _, row in test_df.iterrows():\n",
    "    if row[\"esco_code\"] in esco_code_to_occupation_id:\n",
    "        occupation_id = esco_code_to_occupation_id[row[\"esco_code\"]]\n",
    "        skills_essential.append([skill_id_to_uuid[skill_id] for skill_id in occupation_id_to_skills_essential.get(occupation_id,[])])\n",
    "        skills_optional.append([skill_id_to_uuid[skill_id] for skill_id in occupation_id_to_skills_optional.get(occupation_id,[])])\n",
    "    else:\n",
    "        missing_occupation_codes.append(row[\"esco_code\"])\n",
    "\n",
    "redacted_test_df = test_df[~test_df[\"esco_code\"].isin(missing_occupation_codes)]\n",
    "redacted_test_df[\"skills_essential\"] = skills_essential\n",
    "redacted_test_df[\"skills_optional\"] = skills_optional\n",
    "\n",
    "print(\"Missing occupation ESCO codes: \", missing_occupation_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tabiya/hahu_test/commit/689243c486d6d30914875ae2c1f984da0e4f23b9', commit_message='Upload redacted_hahu_test_with_id.csv with huggingface_hub', commit_description='', oid='689243c486d6d30914875ae2c1f984da0e4f23b9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import tempfile\n",
    "\n",
    "api = HfApi()\n",
    "with tempfile.NamedTemporaryFile() as temp:\n",
    "    test_df[[\"ID\", \"title\", \"description\", \"general_classification\", \"esco_code\"]].to_csv(temp.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp.name,\n",
    "        path_in_repo=\"hahu_test_with_id.csv\",\n",
    "        repo_id=\"tabiya/hahu_test\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "with tempfile.NamedTemporaryFile() as temp2:\n",
    "    redacted_test_df[[\"ID\", \"title\", \"redacted_description\", \"general_classification\", \"esco_code\", \"synthetic_query\", \"skills_essential\", \"skills_optional\"]].to_csv(temp2.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp2.name,\n",
    "        path_in_repo=\"redacted_hahu_test_with_id.csv\",\n",
    "        repo_id=\"tabiya/hahu_test\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
