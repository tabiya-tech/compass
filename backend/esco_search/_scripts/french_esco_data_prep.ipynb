{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French occupations Data Preparation\n",
    "\n",
    "In order to evaluate the performance of the multilingual embedding model on the French ESCO database, we need to translate our existing Hahu-jobs dataset. This happens because we don't have an available French test set.\n",
    "\n",
    "In our use-case we will only translate the synthetic user query that was previously generated and appended to the data. Moreover, we want to evaluate how this linking performs to our other option of translating the query from French to English and then linking to the English ESCO. In order to do that, we will take the following steps:\n",
    "\n",
    "1. Load the Hahu test dataset from Huggingface;\n",
    "2. Translate the existing `synthetic_query` field into French using Gemini and save it into the `fr_synthetic_query`;\n",
    "3. Translate the `fr_synthetic_query` field back into English using Gemini and save it into the `fr_to_en_synthetic_query` field;\n",
    "4. Save the resulting columns, along with `ID`, `synthetic_query` and `esco_code` in a new file in the Hahu jobs repository on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "OCCUPATION_FILENAME = \"redacted_hahu_test_with_id.csv\"\n",
    "OCCUPATION_REPO_ID = \"tabiya/hahu_test\"\n",
    "\n",
    "df_occupation_test = pd.read_csv(\n",
    "    hf_hub_download(repo_id=OCCUPATION_REPO_ID, filename=OCCUPATION_FILENAME, repo_type=\"dataset\", token=HF_TOKEN)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for machine translation\n",
    "from time import sleep\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "\n",
    "def translate_text(text: str, language_from: str = \"English\", language_to: str = \"French\") -> str:\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init()\n",
    "    # Load the model\n",
    "    generative_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "    # Query the model\n",
    "    response = generative_model.generate_content(\n",
    "        [\n",
    "            f\"Given the following \\\n",
    "            text in {language_from}, return the translation to {language_to}. \\\n",
    "            Answer only with the translated sentence.\\n\\\n",
    "            Text in {language_from}: {text}\\n\\\n",
    "            Text in {language_to}: \",\n",
    "        ]\n",
    "    )\n",
    "    # Introduce a delay to make sure we don't send \n",
    "    # too many requests\n",
    "    sleep(2)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate synthetic querys to French and back to English for linking\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_occupation_test[\"synthetic_query\"] = df_occupation_test[\"synthetic_query\"].apply(str)\n",
    "df_occupation_test[\"fr_synthetic_query\"] = df_occupation_test[\"synthetic_query\"].progress_apply(translate_text)\n",
    "df_occupation_test[\"fr_to_en_synthetic_query\"] = df_occupation_test[\"fr_synthetic_query\"].progress_apply(lambda x: translate_text(x, \"French\", \"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resulting columns to Huggingface\n",
    "from huggingface_hub import HfApi\n",
    "import tempfile\n",
    "\n",
    "api = HfApi()\n",
    "with tempfile.NamedTemporaryFile() as temp:\n",
    "    df_occupation_test[[\"ID\", \"synthetic_query\", \"fr_synthetic_query\", \"fr_to_en_synthetic_query\", \"esco_code\"]].to_csv(temp.name)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=temp.name,\n",
    "        path_in_repo=\"synthetic_queries_translated.csv\",\n",
    "        repo_id=\"tabiya/hahu_test\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=HF_TOKEN\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
